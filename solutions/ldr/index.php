<?php
$title = "Grid Solutions - Data Replication in LIGO";
$section = "section-5";
include_once( "../../include/local.inc" );
include_once( $SITE_PATHS["SERV_INC"].'header.inc' ); 
include_once($SITE_PATHS["SERV_INC"] . "app-info.inc");
?>
<!--<div id="left">
<?php include($SITE_PATHS["SERV_INC"].'solutions.inc'); ?>
</div>
-->

<div id="main">

<h1 class="first">Making Copies for LIGO<h1>

<p>
The Laser Interferometer Gravitational Wave Observatory (LIGO) is a multi-site 
national research facility whose objective is the detection of gravitational waves.
LIGO consists of two facilities, one in Livingston, LA, and one in Richland, WA.
The LIGO detectors at these two sites work together to detect gravitational waves:
tiny distortions of space and time caused when very large masses, such as stars, 
move suddenly. The LIGO detectors are used together and sometimes in cooperation with 
detectors in other countries to look for coincident signals that may be indications
of gravitational waves.
</p>

<p>
The LIGO facility includes three interferometers at the two sites and records 
thousands of channels at several sampling rates. This results in approximately one
terabyte (1 TB = 1,024 GB = 1,048,576 MB) of data every day during a detection run.
These runs typically last for 2-8 weeks. Because the LIGO detector sites are quite 
remote, the data is stored on tapes and shipped to CalTech where it is reduced to 
approximately 200 GB/day and made available online.
</p>

<p>
The data generated by a LIGO run must be scientifically analyzed for it to be of
any value. The analysis that must be performed is computationally intensive (it
can require hundreds of Teraflops), and of course the data volume itself is quite 
large. The LIGO collaboration currently includes computing facilities at nine sites
(and growing), mainly because it is not expensive to build an analysis center and
several institutions wanted to do it. Furthermore, the scientists who have the 
expertise to perform this analysis are spread across 41 institutions on several 
continents (also growing all the time).  The key challenge for LIGO is to get the
data from the LIGO detectors to the sites where analysis happens and to make those
sites accessible to the participating scientists.
</p>

<p>
The computational challenge faced by LIGO is therefore to replicate approximately
200 GB/day to multiple sites securely (ensuring data integrity), efficiently,
robustly, and (most importantly) in an automated fashion without immediate human 
supervision, and then to use the data in a multitude of independent analysis runs. 
The multiples sites each use different large-scale storage models.  Scientists 
and analysts need a coherent mechanism to learn which data is currently available, 
where it is, and how to get it.  More specific requirements included the following.
</p>

<ul>
<li>High-bandwidth network links (10+ Gb/s) are utilized efficiently.</li>
<li>All network links are used efficiently (minimizing idle time).</li>
<li>Data can be located and understood using application-level terms (metdata).</li>
<li>Replicas (copies) can be located via database queries.</li>
<li>Data transfer endpoints are authenticated using "strong" security and
data integrity is maintained during transfers.</li>
</ul>

<p>
To meet this challenge, the LIGO partnership (a cooperation between 
physicists, computer scientists, and information technology experts)
developed the Lightweight Data Replicator (LDR), an integrated solution
that combines several basic Grid components with other tools to provide
an end-to-end system for managing LIGO's data. By April 2005, the LIGO 
team had used LDR to replicate 20 terabytes of data from CalTech to the 
University of Wisconsin Milwaukee, and had just deployed the software 
at MIT, Penn State University, the Albert Eistein Institute (AEI) in
Germany, and Cardiff University, Wales.
</p>

<p>
LDR combines the 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."security/pre-ws-aa.php"; ?>">Grid Security Infrastructure (GSI)</a>, 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."data/gridftp.php"; ?>">GridFTP</a>, 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."data/rls.php"; ?>">Globus Replica Location Service (RLS)</a>, and
<a href="http://dsd.lbl.gov/gtg/projects/pyGlobus/">PyGlobus</a>, with customized Python daemon code 
to provide a solution that meets the requirements above.  The developers plan to integrate the 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."data/mcs.php"; ?>">Metadata Catalog Service (MCS)</a> in the future.
</p>

<?

$software = "<a href='http://www.lsc-group.phys.uwm.edu/LDR/'>Lightweight Data Replicator</a>";
$developer = "<a href='http://www.griphyn.org/'>GriPhyN</a>, <a href='http://www.ivdgl.org/'>iVDGL</a>";
$distros = "Download from <a href='http://www.lsc-group.phys.uwm.edu/LDR/doc/install.html'>Univ. of 
Wisconsin - Milwaukee";
$contact = "<a href='mailto:ldr-discuss@gravity.phys.uwm.edu'>ldr-discuss@gravity.phys.uwm.edu</a><br>
(must be <a href='http://www.lsc-group.phys.uwm.edu/mailman/listinfo/ldr-discuss'>subscribed</a> 
before posting messages)</a>";

app_info($software, $developer, $distros, $contact);

?>

<p></p>

</div>

<?

include($SITE_PATHS["SERV_INC"].'footer.inc');

?>
