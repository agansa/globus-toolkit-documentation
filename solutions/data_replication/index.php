<?php
$title = "Grid Solutions - Data Replication for LIGO";
$section = "section-5";
include_once( "../../include/local.inc" );
include_once( $SITE_PATHS["SERV_INC"].'header.inc' ); 
include_once($SITE_PATHS["SERV_INC"] . "app-info.inc");
?>
<!--<div id="left">
<?php include($SITE_PATHS["SERV_INC"].'solutions.inc'); ?>
</div>
-->

<div id="main">

<h1 class="first">Large-scale Data Replication for LIGO</h1>

<div style="float: right; margin-left: 10px; margin-bottom: 10px;">
<img border=0 src="ligo-livingston.jpg" width=406 height=275>
</div>

<p>
The Laser Interferometer Gravitational Wave Observatory (LIGO) is a multi-site 
national research facility whose objective is the detection of gravitational waves.
LIGO consists of two facilities, one in Livingston, LA, and one in Richland, WA,
operated jointly by the California Institute of Technology (CalTech) and the
Massachusetts Institute of Technology (MIT).
The LIGO detectors at these two sites work together to detect gravitational waves:
tiny distortions of space and time caused when very large masses, such as stars, 
move suddenly. The LIGO detectors are used together and in cooperation with 
detectors in other countries to look for coincident signals that may be indications
of gravitational waves.
</p>

<h2>The Challenge</h2>

<p>
The LIGO facility includes three interferometers at the two sites and records 
thousands of channels at several sampling rates. This results in approximately one
terabyte (1 TB = 1,024 GB = 1,048,576 MB) of data every day during a detection run.
During the commissioning phase of the LIGO interferometers, these runs typically lasted
for 2-8 weeks. LIGO expects to begin a one-year detection run late in 2005.  Because 
the LIGO detector sites are remote, the data was originally stored on tapes and shipped 
to CalTech where it was made available online to scientists.  More recently, upgraded
network connections have allowed replication to other data centers directly from the 
detector sites.
</p>

<p>
The data generated by a LIGO run must be scientifically analyzed for it to be of
any value. The analysis is computationally intensive (some classes of astrophysical 
searches can require hundreds of Teraflops), and the data volume itself is quite 
large. Nine sites within the LIGO collaboration (each operated independently) currently
provide computing facilities based on commodity cluster computing. The scientists who have the 
expertise to perform this analysis are spread across 41 institutions on several 
continents, and this community is growing all the time.  The key challenge for LIGO 
is to get the data from the LIGO detectors to the sites where analysis happens and to 
make those sites accessible to the participating scientists.
</p>

<div id="solutions-sidebar" style="float: right; margin-right: 1em">
<h2>More Information</h2>
<ul>
<li><a href="http://www.ligo.caltech.edu/LIGO_web/about/">LIGO Overview</a></li>
<li><a href="architecture.php">System Architecture</a></li>
<li><a href="resources.php">Software Availability</a></li>
<li><a href="http://www.lsc-group.phys.uwm.edu/LDR/">Lightweight Data Replicator</a></li>
<li><a href="<?=$SITE_PATHS["WEB_TOOLKIT"]."docs/4.0/techpreview/datarep/"; ?>">Data Replication Service</a></li>
</ul>
</div>

<p>
The data management challenge faced by LIGO is therefore to replicate approximately
1 TB/day of data to multiple sites securely, efficiently, robustly, and 
automatically; to keep track of where replicas have been made for each piece of the
data; and to use the data in a multitude of independent analysis runs. 
The nine sites each use mass storage systems, but different systems are used at
different sites. Scientists and analysts need a coherent mechanism to learn which 
data items are currently available, where they are, and how to access them.  
More specific requirements include the following.
</p>

<ul>
<li>When high-bandwidth network links (10+ Gb/s) are available, they should be 
    utilized efficiently: there should not be unused bandwidth while data transfers are 
    taking place.</li>
<li>All network links should be used efficiently: there should be no idle time 
    on the network between transfers.</li>
<li>Scientists should be able to to locate data and understand data items using 
    application-level terms (also known as "metadata").</li>
<li>Scientists should be able to locate replicas (copies) of any data item using
    database queries.</li>
<li>Data transfer endpoints should be authenticated using "strong" security, and
    data transfers should preserve data integrity.</li>
</ul>

<h2>The Solution</h2>

<p>
To meet this challenge, the LIGO Scientific Collaboration (a cooperation between 
physicists, computer scientists, and information technology experts)
developed the <a href="http://www.lsc-group.phys.uwm.edu/LDR/">Lightweight 
Data Replicator (LDR)</a>, an integrated solution that combines several 
basic Grid components with other tools to provide an end-to-end system 
for managing LIGO's data. LDR's features make it very useful for
replicating data sets to multiple sites within a joint project.
</p>

<ul>
<li>LDR is intended to be the simplest, easiest-to-maintain software that meets the LIGO requirements. It is based on existing open source Grid components, adding control logic to orchestrate the replication tasks.</li>
<li>LDR uses network links efficiently because it uses parallel data streams, tunable TCP windows, and tunable write/read buffers. LDR also manages continuous data transfers between sites, ensuring that the network is always being used when there are transfers that need to happen.</li>
<li>LDR tracks where copies of specific files can be found, given that it may have been replicated at several sites. This information can be queried by user applications so that local data is used when it is available. Each site maintains a catalog of the data it contains, which can be queried directly. The catalogs update each other so that so that one can query any member site's catalog to locate a particular file.</li>
<li>LDR also stores descriptive information (metadata) in a database. Hence, site administrators can select groups of files for replication based on descriptive fields rather than having to specify the name of every file.</li>
<li>LDR uses the Grid Security Infrastructure (GSI) to authenticate clients and services.</li>
</ul>

<p>
LDR combines several existing Grid components--
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."security/pre-ws-aa.php"; ?>">Grid Security Infrastructure (GSI)</a>, 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."data/gridftp.php"; ?>">GridFTP</a>, 
<a href="<?=$SITE_PATHS["WEB_SOFTWARE"]."data/rls.php"; ?>">Globus Replica Location Service (RLS)</a>, 
a metadata catalog service, and
<a href="http://dsd.lbl.gov/gtg/projects/pyGlobus/">PyGlobus</a>--with customized Python daemon code 
to provide a solution that meets the requirements above.
</p>

<h2>Results</h2>

<p>
The LIGO detectors went online and began producing data in 2002.  By April 
2005, the LIGO team had used LDR to replicate well over 50 terabytes of 
data to sites including CalTech, MIT, Penn State University, and the University 
of Wisconsin Milwaukee, along with sites in Europe including the Albert Einstein 
Institute in Golm, Germany, Cardiff University, and the University of 
Birmingham.
</p>

<p>LDR was developed by Scott Koranda, Brian Moe, and Kevin Flasch at the 
University of Wisconsin-Milwaukee in cooperation with the NSF-funded 
GriPhyN and iVDGL projects and the DOE-funded SciDAC Data Grid Middleware 
project. Many members of the LIGO Scientific Collaboration have contributed 
to deploying and testing LDR.</p>

<p>Subsequent to the successes with LDR in the LIGO project, members of the
Globus Alliance have designed and implemented a 
<a href="<?=$SITE_PATHS["WEB_TOOLKIT"]."docs/4.0/techpreview/datarep/"; ?>">Data Replication Service (DRS)</a>
that provides a pull-based replication capability similar to that provided in 
the LIGO LDR system. The function of the DRS is to ensure that a specified set 
of files exists on a storage site by comparing the contents of an RLS local 
file catalog with a list of desired files, transferring copies of the missing 
files from other locations and registering them in the local file catalog. The 
DRS is implemented as a Web service that complies with the Web Services 
Resource Framework (WS-RF) specifications. The DRS is available as a technology
preview in the Globus Toolkit 4.0.</p>

<h2>Detailed Information</h2>

<p>The following links provide more detail about the Lightweight 
Data Replicator (LDR).</p>

<ul>
<li><a href="architecture.php">Architecture of the data replication system</a></li>
<li><a href="resources.php">Software availability</a></li>
<li><a href="http://www.lsc-group.phys.uwm.edu/LDR/">Lightweight Data Replicator website</a></li>
<li><a href="http://www.ligo.caltech.edu/LIGO_web/about/">Overview of the LIGO facility</a></li>
</ul>

</div>

<?
include($SITE_PATHS["SERV_INC"].'footer.inc');
?>
