<para>
<emphasis>When I submit a streaming or staging job, I get the following error:

ERROR service.TransferWork Terminal transfer error: [Caused by: Authentication failed[Caused by: Operation unauthorized(Mechanism level: Authorization failed. Expected"/CN=host/localhost.localdomain" target but received "/O=Grid/OU=GlobusTest/OU=simpleCA-my.machine.com/CN=host/my.machine.com")
</emphasis>
</para>
<itemizedlist>
    <listitem><para>Check <filename>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</filename>  
        to see if it uses <computeroutput>localhost</computeroutput> or <computeroutput>127.0.0.1</computeroutput> instead of the public
    hostname (in the example above, <computeroutput>my.machine.com</computeroutput>).
    Change these uses of the loopback hostname or IP to the public hostname as
    neccessary.</para></listitem>
</itemizedlist>

<para>
<emphasis>Fork jobs work fine, but submitting PBS jobs with globusrun-ws hangs at "Current job state: Unsubmitted"</emphasis>
</para>
<orderedlist>
    <listitem><para>Make sure the log_path in
    <filename>$GLOBUS_LOCATION/etc/globus-pbs.conf</filename> points to locally accessible scheduler
    logs that are readable by the user running the container.  The Scheduler
    Event Generator (SEG) will not work without local scheduler logs to monitor.
    This can also apply to other resource managers, but is most comonly seen
    with PBS.
    </para></listitem>
    <listitem><para>If the SEG configuration looks sane, try running the SEG
    tests. They are located in
    <filename>$GLOBUS_LOCATION/test/globus_scheduler_event_generator_*_test/</filename>. If Fork jobs
    work, you only need to run the PBS test. Run each test by going to the
    associated directory and run <filename>./TESTS.pl</filename>. If any tests fail, report this to
        the <ulink url="http://dev.globus.org/wiki/GRAM#Mailing_Lists">gram-dev@globus.org</ulink> mailing list.
    </para></listitem>
    <listitem><para>If the SEG tests succeed, the next step is to figure out
    the ID assigned by PBS to the queued job. Enable GRAM debug logging by
    uncommenting the appropriate line in the
    <filename>$GLOBUS_LOCATION/container-log4j.properties</filename> configuration file.
    Restart the container, run a PBS job, and search the container log for a
    line that contains "Received local job ID" to obtain the local job ID.
    </para></listitem>
    <listitem><para>Once you have the local job ID, you can find out if the PBS status is being logged 
        by checking the latest PBS logs pointed to by the value of "log_path" in
    <filename>$GLOBUS_LOCATION/etc/globus-pbs.conf</filename>.</para> 
        
        <para>If the status is not being logged, check the documentation for your
    flavor of PBS to see if there's any futher configuration that needs to be
    done to enable job status logging. For example, PBS Pro requires a
    sufficient <option>-e &lt;bitmask&gt;</option> option added to the <command>pbs_server</command> command line to
    enable enough logging to satisfy the SEG.
    </para></listitem>
    <listitem><para>If the correct status is being logged, try running the
    SEG manually to see if it is reading the log file properly. The general
    form of the SEG command line is as follows:

    <screen>
    $GLOBUS_LOCATION/libexec/globus-scheduler-event-generator -s pbs -t &lt;timestamp&gt;
    </screen>

    The timestamp is in seconds since the epoch and dictates how far back in the
    log history the SEG should scan for job status events. The command should
    hang after dumping some status data to stdout. </para>
        
        <para>If no data appears, change the timestamp to an earlier time.</para>
            
        <para>If nothing ever appears, report this to the <ulink url="http://dev.globus.org/wiki/GRAM#Mailing_Lists">gram-user@globus.org</ulink> mailing list.</para>
    </listitem>
    <listitem><para>If running the SEG manually succeeds, try running another
    job and make sure the job process actually finishes and PBS has logged the
    correct status before giving up and cancelling globusrun-ws. If things are
    still not working, report your problem and exactly what you have tried to
        remedy the situtation to the <ulink url="http://dev.globus.org/wiki/GRAM#Mailing_Lists">gram-user@globus.org</ulink> mailing list.
    </para></listitem>
</orderedlist>

<para>
<emphasis>The job manager detected an invalid script response</emphasis>
</para>
<itemizedlist>
    <listitem><para>Check for a restrictive umask. When the service writes the native
        scheduler <glossterm linkend="job-description">job description</glossterm> to a file, an overly restrictive umask will
        cause the permissions on the file to be such that the submission
        script run through <glossterm linkend="sudo">sudo</glossterm> as the user cannot read the file (<ulink url="http://bugzilla.globus.org/bugzilla/show_bug.cgi?id=2655">bug #2655</ulink>).
    </para></listitem>
</itemizedlist>

<para>
<emphasis>When restarting the container, I get the following error:
Error getting delegation resource</emphasis>
</para>
<itemizedlist>
    <listitem><para>Most likely this is simply a case of the delegated
    credential expiring.  Either refresh it for the affected job or destroy
        the job resource. For more information, see <ulink url="../../security/delegation/user-index.html#commandline">delegation command-line
            clients</ulink>.
    </para></listitem>
</itemizedlist>


<para>
  <emphasis>The user's home directory has not been determined correctly</emphasis>
</para>
<itemizedlist>
   <listitem><para>
     This occurs when the administrator changed the location of the users'
     home directory and did not restart the GT4 container afterwards. Beginning
     with version 4.0.3, WS-GRAM determines a user's home directory
     only once in the lifetime of a container (when the user submits the first
     job). Subsequently, submitted jobs will use the cached home directory during
     job execution.
   </para></listitem>
</itemizedlist>
