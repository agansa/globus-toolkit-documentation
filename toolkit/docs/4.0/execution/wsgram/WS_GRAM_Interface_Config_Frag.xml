<section id="s-wsgram-admin-configuring-typical"><title>Typical Configuration</title>
    
    <section id="s-wsgram-admin-configsudo"><title>Configuring sudo</title>
        <para>When the credentials of the service account and the job submitter are different
            (multi user mode), then GRAM will prepend a call to sudo to the local adapter
            callout command. <emphasis>Important:</emphasis> If sudo is not configured properly, the command
            and thus job will fail.</para>
        <para>
            As <emphasis>root</emphasis>, here are the two lines to add to the
            /etc/sudoers file for each GLOBUS_LOCATION installation, where
            /opt/globus/GT4.0.0 should be replaced with the GLOBUS LOCATION for your
            installation:
            <screen># Globus GRAM entries
                globus  ALL=(username1,username2) 
                NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
                -g /etc/grid-security/grid-mapfile
                /opt/globus/GT4.0.0/libexec/globus-job-manager-script.pl *
                globus  ALL=(username1,username2) 
                NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
                -g /etc/grid-security/grid-mapfile
                /opt/globus/GT4.0.0/libexec/globus-gram-local-proxy-tool *
            </screen>
            
        </para>
        <para>
            The <computeroutput>globus-gridmap-and-execute</computeroutput> program is
            used to ensure that GRAM only runs programs under accounts that are in the
            grid-mapfile.  In the sudo configuration, it is the first program called.
            It looks up the account in the grid-mapfile and then runs the requested 
            command.  It is redundant if sudo is properly locked down.  This tool 
            could be replaced with your own authorization program.
        </para>
    </section>
    <!-- Configuring sudo -->
    
    <section id="s-wsgram-admin-configuringscheduleradapters"><title>Configuring Scheduler Adapters</title>
        <para>
            The WS GRAM scheduler adapters included in the release tarball
            are: <glossterm linkend="pbs">PBS</glossterm>, <glossterm linkend="condor">Condor</glossterm> and <glossterm linkend="lsf">LSF</glossterm>. To install, follow these steps (shown for pbs):
            
            <screen>    % cd $GLOBUS_LOCATION\gt4.0.0-all-source-installer
                
                % make gt4-gram-pbs
                
                % make install
                
            </screen>
        </para>
        <para>
            Using PBS as the example, make sure the scheduler commands are in your
            path (qsub, qstat, pbsnodes).
        </para>
        <para> For PBS, another setup step is required to configure the remote shell for
            rsh access: 
            
            <screen>
                
                % cd $GLOBUS_LOCATION/setup/globus
                
                % ./setup-globus-job-manager-pbs --remote-shell=rsh
                
            </screen>
        </para>
        <para>
            The last thing is to define the <link linkend="s-wsgram-Interface_Config_Frag-filesysmap">GRAM 
                and GridFTP file system mapping</link> for PBS.  A default
            mapping in this file is created to allow simple jobs to run.  However, the
            actual file system mappings for your compute resource should be entered to
            ensure:
            <itemizedlist>
                <listitem><para>files staging is performed correctly
                </para>
                </listitem>
                <listitem><para>
                    jobs with erroneous file path directives are rejected</para>
                </listitem>
            </itemizedlist>
        </para>
        <para>
            Done! You have added the PBS scheduler adapters to your GT installation.
        </para>
        <para>
            Note for future GT builds with scheduler adapters: scheduler adapters can
            be enabled by adding --enable-wsgram-pbs to the configure line when
            building the entire toolkit.
            <screen>
                % configure --prefix=$GLOBUS_LOCATION --enable-wsgram-pbs ...
                % make
                % make install
            </screen>
        </para>
    </section>
    <!-- enabling scheduler adapters -->
    
</section>
<!-- Typical Configuration -->

<section id="s-wsgram-admin-configuring-nondefault"><title>Non-default Configuration</title>
    
    <section id="s-wsgram-admin-userproxy"><title>Non-default Credentials</title>
        <para>
            To run the container using just a user proxy, instead of host creds,
            edit the
            <computeroutput>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</computeroutput>
            file, and either comment out the credentials section...
            <screen>
                &lt;?xml version="1.0" encoding="UTF-8"?&gt;
                &lt;securityConfig xmlns="http://www.globus.org"&gt;
                &lt;!--
                &lt;credential&gt;
                &lt;key-file value="/etc/grid-security/containerkey.pem"/&gt;
                &lt;cert-file value="/etc/grid-security/containercert.pem"/&gt;
                &lt;credential&gt;
                --&gt;
                &lt;gridmap value="/etc/grid-security/grid-mapfile"/&gt;
                &lt;securityConfig&gt;
            </screen>
            or replace the credentials section with a proxy file location...
            <screen>
                &lt;?xml version="1.0" encoding="UTF-8"?&gt;
                &lt;securityConfig xmlns="http://www.globus.org"&gt;
                &lt;proxy-file value="&lt;PATH TO PROXY FILE&gt;"/&gt;
                &lt;gridmap value="/etc/grid-security/grid-mapfile"/&gt;
                &lt;securityConfig&gt;
            </screen>
            Running in personal mode (user proxy), another GRAM configuration setting
            is required. For GRAM to authorize the RFT service when performing
            staging functions, it needs to know the subject DN for verification. Here are
            the steps:
            <screen>
                % cd $GLOBUS_LOCATION/setup/globus
                % ./setup-gram-service-common --staging-subject=
                "/DC=org/DC=doegrids/OU=People/CN=Stuart Martin 564720"
            </screen>
            You can get your subject DN by running this command:
            <screen>
                % grid-cert-info -subject
            </screen>
        </para>
    </section>
    <!-- Non-default Credentials -->
    
    <section id="s-wsgram-admin-nondefaultgridftp" xreflabel="Non-default GridFTP server"><title>Non-default GridFTP server </title>
        <para>
            By default, the GridFTP server is assumed to run as root on localhost:2811.
            If this is not true for your site then change it by editing the GridFTP host
            and/or port in the <link linkend="s-wsgram-Interface_Config_Frag-filesysmap">GRAM and GridFTP file system mapping</link> config file: <computeroutput>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</computeroutput>.
        </para>
        
    </section>
    <!-- Non-default GridFTP server -->
    
    <section id="s-wsgram-admin-nondefaultcontainerport"><title>Non-default container port</title>
        <para>
            By default, the globus services will assume 8443 is the port the Globus
            container is using.  However the container can be run under a non-standard
            port, for example:
            <screen>
                % globus-start-container -p 4321
            </screen>
        </para>
    </section>
    <!-- Non-default container port -->
    
    <section id="s-wsgram-admin-nondefaultgridmap"><title>Non-default gridmap</title>
        <para>
            If you wish to specify a non-standard gridmap file in a multi-user
            installation, two basic configurations need to be changed:</para>
        
        <itemizedlist>
            <listitem><para>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml
                <itemizedlist><listitem><simpara>As specified in the <ulink url="http://www.globus.org/toolkit/docs/4.0/security/authzframe/security_descriptor.html#s-authzframe-secdesc-configGridmap">gridmap config</ulink> instructions, add a &lt;gridmap value="..."/&gt;
                    element to the file appropriately.</simpara></listitem>
                </itemizedlist>
            </para></listitem>
            
            <listitem><para>/etc/sudoers
                <itemizedlist>
                    <listitem><para>Change the file path after all -g options<screen>-g /path/to/grid-mapfile</screen>.
                    </para></listitem>
                </itemizedlist>
            </para></listitem>
        </itemizedlist>
        
        <para>
            Example:
            
            <emphasis>global_security_descriptor.xml</emphasis>
            <screen>
                ...
                
                &lt;gridmap value="/opt/grid-mapfile"/&gt;
                
                ...
            </screen>
            <emphasis>sudoers</emphasis>
            <screen>
                ...
                
                # Globus GRAM entries
                globus  ALL=(username1,username2) 
                NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
                -g /opt/grid-mapfile
                /opt/globus/GT4.0.0/libexec/globus-job-manager-script.pl *
                globus  ALL=(username1,username2) 
                NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
                -g /opt/grid-mapfile
                /opt/globus/GT4.0.0/libexec/globus-gram-local-proxy-tool *
                
                ...
            </screen>
        </para>
    </section>
    <!-- Non-default gridmap file -->
    
    <section id="s-wsgram-admin-nondefaultrft"><title>Non-default RFT deployment</title>
        <para>
            RFT is used by GRAM to stage files in and out of the job execution environment.
            In the default configuration, RFT is hosted in the same container as GRAM and
            is assumed to have the same service path and standard service names. This need
            not be the case.  For example, the most likely alternative scenario is that RFT
            would be hosted seperately in a container on a different machine.  In any case,
            both the RFT and the Delegation Service endpoints need to be adjustable to allow
            this flexibility.  The following options can be passed to the
            <emphasis>setup-gram-service-common</emphasis> script to affect these settings:
        </para>
        <screen>
            --staging-protocol=&lt;protocol&gt;
            --staging-host=&lt;host&gt;
            --staging-port=&lt;port&gt;
            --staging-service-path=&lt;RFT and Delegation factory service path&gt;
            --staging-factory-name=&lt;RFT factory service name&gt;
            --staging-delegation-factory-name=&lt;name of Delegation factory service used by RFT&gt;
        </screen>
        <simpara>for example</simpara>
        <screen>
            % setup-gram-service-common \
            --staging-protocol=http
            --staging-host=somemachine.fakedomain.net
            --staging-port=8444
            --staging-service-path=/tomcat/services/
            --staging-factory-name=MyReliableFileTransferFactoryService
            --staging-delegation-factory-name=MyDelegationFactoryServiceForRFT
        </screen>
        <simpara>will internally cause the GRAM service code to construct the following
            EPR addresses:</simpara>
        <screen>
            http://somemachine.fakedomain.net:8444/tomcat/services/MyReliableFileTransferFactoryService
            
            http://somemachine.fakedomain.net:8444/tomcat/services/MyDelegationFactoryServiceForRFT
        </screen>
    </section> 
    <!-- Non-default RFT deployment -->
    
</section>
<!-- Non-default Configuration -->

<section id="s-wsgram-Interface_Config_Frag-configfilelocation"><title>Locating configuration files</title>
<para>
All the GRAM service configuration files are located in subdirectories of 
the <computeroutput>$GLOBUS_LOCATION/etc</computeroutput> directory. The names of the GRAM 
configuration directories all start with <computeroutput>gram-service</computeroutput>.
For instance, with a default GRAM installation, the command line:
<screen>
% ls etc | grep gram-service
</screen>

gives the following output:

<screen>
gram-service
gram-service-Fork
gram-service-Multi
</screen>
</para>
</section>

<section id="s-wsgram-Interface_Config_Frag-serverconfig"><title>Web service deployment configuration</title>
<para>
The file <computeroutput>$GLOBUS_LOCATION/etc/gram-service/server-config.wsdd</computeroutput>
contains information necessary to deploy and instantiate the GRAM
services in the Globus container.
</para><para>
Three GRAM services are deployed:</para>
<itemizedlist>
<listitem><simpara>ManagedExecutableJobService: service invoked when querying or
managing an <emphasis>executable job</emphasis> </simpara></listitem>

<listitem><simpara>ManagedMultiJobService: service invoked when querying or managing
    a <glossterm linkend="multijob">multijob</glossterm> </simpara></listitem>

<listitem><simpara>ManagedJobFactoryService: service invoked when submitting a job
</simpara></listitem>

</itemizedlist>
<para>
Each service deployment information contains the name of the Java
service implementation class, the path to the WSDL service file, the
name of the operation providers that the service reuses for its
implementation of WSDL-defined operations, etc. More information about
the service deployment configuration information can be found <ulink
url="http://www.globus.org/toolkit/docs/4.0/common/javawscore/Java_WS_Core_Public_Interfaces.html#config">here</ulink>.
</para>
</section>

<section id="s-wsgram-Interface_Config_Frag-jndiconfig"><title>JNDI application configuration</title>
<para>
The configuration of WSRF resources and application-level service
configuration not related to service deployment is contained in <ulink
url="http://java.sun.com/products/jndi/">JNDI</ulink> files.  The
JNDI-based GRAM configuration is of two kinds:
</para>

<section id="s-wsgram-Interface_Config_Frag-commonfactoryconfig"><title>Common job factory configuration</title>
<para>
The file <computeroutput>$GLOBUS_LOCATION/etc/gram-service/jndi-config.xml</computeroutput> contains 
configuration information that is common to every local resource manager.
</para><para>
More precisely, the configuration data it contains pertains to the implementation 
of the GRAM WSRF resources (factory resources and job resources), 
as well as initial values of WSRF resource properties that are always 
published by any Managed Job Factory WSRF resource. 
</para><para>
The data is categorized by service, because according to WSRF, 
in spite of the service/resource separation of concern, a given service 
will use only one XML Schema type of resource. In practice it is therefore clearer
to categorize the configuration resource implementation by service, 
even if theoretically speaking a given resource implementation could be 
used by several services. For more information, refer to the <ulink url="http://www.globus.org/toolkit/docs/4.0/common/javawscore/index.html">Java
WS Core documentation</ulink>.
 </para><para>
Here is the decomposition, in JNDI objects, of the common configuration 
data, categorized by service. Each XYZHome object contains the same 
Globus Core-defined information for the implementation of the WSRF resource, 
such as the Java implementation class for the resource (<computeroutput>resourceClass</computeroutput> datum), 
the Java class for the resource key (<computeroutput>resourceKeyType</computeroutput> datum), etc. 
</para>
<itemizedlist>
<listitem><para>ManagedExecutableJobService
   <itemizedlist>
     <listitem><simpara>ManagedExecutableJobHome: configuration of the implementation of resources for the service.</simpara></listitem>
   </itemizedlist>
</para></listitem>

<listitem><para>ManagedMultiJobService
   <itemizedlist>
     <listitem><simpara>ManagedMultiJobHome: configuration of the implementation of resources for the service</simpara></listitem>
   </itemizedlist>
</para></listitem>

<listitem><para>ManagedJobFactoryService
   <itemizedlist>
   <listitem><simpara>FactoryServiceConfiguration: this encapsulates configuration information 
       used by the factory service. Currently this identifies the service to associate 
       to a newly created job resource in order to create an endpoint reference and 
       return it.
   </simpara></listitem>
   
   <listitem><simpara>ManagedJobFactoryHome: implementation of resources for the service
      resourceClass</simpara></listitem>
      
   <listitem><simpara>FactoryHomeConfiguration: this contains GRAM application-level configuration data 
       i.e. values for resource properties common to all factory resources. For instance, 
       the path to the Globus installation, host information such as CPU type, manufacturer, 
       operating system name and version, etc.</simpara></listitem>
   </itemizedlist>
</para></listitem>
 
</itemizedlist>     

</section>

<section id="s-wsgram-Interface_Config_Frag-managerconfig"><title>Local resource manager configuration</title>

<para>When a SOAP call is made to a GRAM factory service in order to submit a job, 
the call is actually made to a GRAM service-resource pair, where the 
factory resource represents the local resource manager to be used to execute the job.</para>
<para>
There is one directory <computeroutput>gram-service-&lt;manager&gt;/</computeroutput> 
for each local resource manager supported by the GRAM installation. 
</para>
<para>For instance, let's assume the command line:
<screen>
% ls etc | grep gram-service-
</screen>

gives the following output:

<screen>
gram-service-Fork
gram-service-LSF
gram-service-Multi
</screen>
</para>
    <para>In this example, the Multi, Fork and <glossterm linkend="lsf">LSF</glossterm> job factory resources have been 
installed. <computeroutput>Multi</computeroutput> is a special kind of local resource manager 
    which enables the GRAM services to support <ulink url="http://www.globus.org/toolkit/docs/4.0/execution/wsgram/user-index.html#s-wsgram-user-specifyingmultijob">multijobs</ulink>.</para>


<para>The JNDI configuration file located under each manager directory 
contains configuration information for the GRAM support of the given 
local resource manager, such as the name that GRAM uses to designate 
the given resource manager. This is referred to as the <emphasis>GRAM name</emphasis> 
of the local resource manager.</para><para>
For instance, <computeroutput>$GLOBUS_LOCATION/etc/gram-service-Fork/jndi-config.xml</computeroutput> 
contains the following XML element structure:</para>
<screen>
    &lt;service name="ManagedJobFactoryService"&gt;
        &lt;!-- LRM configuration:  Fork --&gt;
        &lt;resource
            name="ForkResourceConfiguration"
            type="org.globus.exec.service.factory.FactoryResourceConfiguration"&gt;
            &lt;resourceParams&gt;
                [...]
                &lt;parameter&gt;
                    &lt;name&gt;
                        <emphasis>localResourceManagerName</emphasis>
                    &lt;/name&gt;
                    &lt;value&gt;
                        <emphasis>Fork</emphasis>
                    &lt;/value&gt;
                &lt;/parameter&gt;           
                &lt;!-- Site-specific scratchDir
Default: ${GLOBUS_USER_HOME}/.globus/scratch
                &lt;parameter&gt;
                    &lt;name&gt;
                        <emphasis>scratchDirectory</emphasis>
                    &lt;/name&gt;
                    &lt;value&gt;
                        <emphasis>${GLOBUS_USER_HOME}.globus/scratch</emphasis>
                    &lt;/value&gt;
                &lt;/parameter&gt;           
                --&gt;
            &lt;/resourceParams&gt;
        &lt;/resource&gt;        
    &lt;/service&gt;
</screen>
<para>In the example above, the name of the local resource manager is
<computeroutput>Fork</computeroutput>.  This value can be used with the GRAM
command line client in order to specify which factory resource to use when
submitting a job.  Similarly, it is used to create an endpoint reference to the
chosen factory WS-Resource when using the GRAM client API. </para>
<para>In the example above, the <emphasis>scratchDirectory</emphasis> is set to
<computeroutput>${GLOBUS_USER_HOME}/.globus/scratch</computeroutput>.  This is
the default setting. It can be configured to point to an alternate file system
path that is common to the compute cluster and is typically less reliable (auto
purging), while offering a greater amount of disk space (thus "scratch").
</para>
</section>
</section>

<section id="s-wsgram-Interface_Config_Frag-securitydesc"><title>Security descriptor</title>
<para>
The file <computeroutput>$GLOBUS_LOCATION/etc/gram-service/managed-job-factory-security-config.xml</computeroutput> contains the Core security configuration for the GRAM <computeroutput>ManagedJobFactory</computeroutput> service:
<itemizedlist>
   <listitem><para>default security information for all remote invocations, such as:
       <itemizedlist>
	  <listitem><simpara>the authorization method, based on a Gridmap file (in order to resolve user credentials to local user names) </simpara></listitem>
	  <listitem><simpara>limited proxy credentials will be rejected</simpara></listitem>
       </itemizedlist></para></listitem>
   <listitem><simpara>security information for the <computeroutput>createManagedJob</computeroutput> operation</simpara></listitem>   
</itemizedlist>

The file <computeroutput>$GLOBUS_LOCATION/etc/gram-service/managed-job-security-config.xml</computeroutput> contains the Core security configuration for the GRAM job resources:
<itemizedlist>
    <listitem><simpara>The default is to only allow the identity that called the createManagedJob operation to access the resource.
    </simpara></listitem>
</itemizedlist>
</para>

<para>
Note: GRAM does not override the container security credentials defined in <computeroutput>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</computeroutput>.  These are the credentials used to authenticate all service requests.
</para>
</section>

<section id="s-wsgram-Interface_Config_Frag-filesysmap"><title>GRAM and GridFTP file system mapping</title>
<para>The file <computeroutput>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</computeroutput> 
contains information to associate local resource managers with GridFTP servers.  GRAM uses the GridFTP server (via RFT) to perform all file staging directives.
Since the GridFTP server and the Globus service container can be run on separate hosts, a mapping is needed between the common file system paths of these 2 hosts.
This enables the GRAM services to resolve file:/// staging directives to the local GridFTP URLs.
</para><para>

Below is the default Fork entry.  Mapping a jobPath of / to ftpPath of / will allow any file staging directive to be attempted.</para>
<screen>
    &lt;map&gt;
        &lt;scheduler&gt;Fork&lt;/scheduler&gt;
        &lt;ftpServer&gt;
           &lt;protocol&gt;gsiftp&lt;/protocol&gt;
           &lt;host&gt;myhost.org&lt;/host&gt;
           &lt;port&gt;2811&lt;/port&gt;
        &lt;/ftpServer&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/&lt;/jobPath&gt;
           &lt;ftpPath&gt;/&lt;/ftpPath&gt;
        &lt;/mapping&gt;
    &lt;/map&gt;
</screen>
<para>
    For a <glossterm linkend="scheduler">scheduler</glossterm>, where jobs will typically run on a compute node, a default entry is not provided.  This means staging directives will fail until a mapping is entered.
    Here is an example of a compute cluster with <glossterm linkend="pbs">PBS</glossterm> installed that has 2 common mount points between the front end host and the GridFTP server host.
</para>
<screen>
    &lt;map&gt;
        &lt;scheduler&gt;PBS&lt;/scheduler&gt;
        &lt;ftpServer&gt;
           &lt;protocol&gt;gsiftp&lt;/protocol&gt;
           &lt;host&gt;myhost.org&lt;/host&gt;
           &lt;port&gt;2811&lt;/port&gt;
        &lt;/ftpServer&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/pvfs/mount1/users&lt;/jobPath&gt;
           &lt;ftpPath&gt;/pvfs/mount2/users&lt;/ftpPath&gt;
        &lt;/mapping&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/pvfs/jobhome&lt;/jobPath&gt;
           &lt;ftpPath&gt;/pvfs/ftphome&lt;/ftpPath&gt;
        &lt;/mapping&gt;
    &lt;/map&gt;
</screen>
<para>
The file system mapping schema doc is <ulink url="http://www.globus.org/toolkit/docs/4.0/execution/wsgram/schemas/gram_fs_map.html">here</ulink>.
</para>
</section>

<section id="s-wsgram-Interface_Config_Fragscheduler_specific_config"><title>Scheduler-Specific Configuration Files</title>
<para>In addition to the service configuration described above, there are
scheduler-specific configuration files for the Scheduler Event Generator
modules. These files consist of name=value pairs separated by newlines.
These files are:
</para>

<table><title>Scheduler-Specific Configuration Files</title>
<tgroup cols="2"><tbody>

  <row><entry>$GLOBUS_LOCATION/etc/globus-fork.conf</entry>
      <entry><para>Configuration for the Fork <glossterm linkend="seg">SEG</glossterm> module implementation. The attributes
      names for this file are:
<variablelist><varlistentry><term>log_path</term><listitem><simpara>
      Path to the SEG Fork log (used by the globus-fork-starter and
      the SEG). The value of this should be the path to a world-writable file.
      The default value for this created by the Fork setup package is 
      $GLOBUS_LOCATION/var/globus-fork.log. This file must be readable by the
      account that the SEG is running as.</simpara></listitem></varlistentry>
</variablelist>
</para></entry></row>

  <row><entry>$GLOBUS_LOCATION/etc/globus-condor.conf</entry>
      <entry><para>Configuration for the <glossterm linkend="condor">Condor</glossterm> SEG module implementation. The attributes
      names for this file are:
<variablelist><varlistentry><term>log_path</term><listitem><simpara>Path to the SEG Condor log (used by the
      Globus::GRAM::JobManager::condor perl module and Condor
      SEG module. The value of this should be the path to a world-readable and
      world-writable file.  The default value for this created by the Fork
      setup package is $GLOBUS_LOCATION/var/globus-condor.log</simpara></listitem></varlistentry>
</variablelist>
  </para></entry>
</row>

  <row><entry>$GLOBUS_LOCATION/etc/globus-pbs.conf</entry>
  <entry><para>Configuration for the PBS SEG module implementation. The attributes
      names for this file are:
<variablelist><varlistentry><term>log_path</term><listitem><simpara>
      Path to the SEG PBS logs (used by the
      Globus::GRAM::JobManager::pbs perl module and PBS
      SEG module. The value of this should be the path to the directory
      containing the server logs generated by PBS. For the SEG to operate,
      these files must have file permissions such that the files may be read
      by the user the SEG is running as.
      </simpara></listitem></varlistentry>
</variablelist>
  </para></entry>
</row>

  <row><entry>$GLOBUS_LOCATION/etc/globus-lsf.conf</entry>
  <entry><para>Configuration for the LSF SEG module implementation. The attributes
      names for this file are:
<variablelist><varlistentry><term>log_path</term><listitem><simpara>
Path to the SEG LSF log directory. This is used by the LSF
      SEG module. The value of this should be the path to the directory
      containing the server logs generated by LSF. For the SEG to operate,
      these files must have file permissions such that the files may be read
      by the user the SEG is running as.
      </simpara></listitem></varlistentry>
</variablelist>
  </para></entry>
</row>
</tbody></tgroup>
</table>
</section>

<section id="s-wsgram-Interface_Config_Frag-autoregistration"><title>WS GRAM auto-registration with default WS MDS Index Service</title>
    <para>With a default GT 4.0.1 installation, the WS GRAM service is automatically registered with the default <ulink url="http://www.globus.org/toolkit/docs/4.0/info/index/">WS MDS Index Service</ulink> running in the same container for monitoring 
        and discovery purposes.</para>
    
    <note><simpara> If you are using GT 4.0.0, we strongly recommend upgrading to 4.0.1 to take advantage of this capability. </simpara></note>
    <para>   However, if must use GT 4.0.0, or if this registration was turned off and you want to turn it back on, this is how it is configured:</para>
    
    <para>There is a jndi resource defined in <filename>$GLOBUS_LOCATION/etc/gram-service/jndi-config.xml</filename> as follows :</para>
    <screen> 
        &lt;resource name="mdsConfiguration" 
        
        type="org.globus.wsrf.impl.servicegroup.client.MDSConfiguration"&gt;
        &lt;resourceParams&gt;
        &lt;parameter&gt; 
        &lt;name&gt;reg&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
        &lt;/parameter&gt;
        &lt;parameter&gt; 
        &lt;name&gt;factory&lt;/name&gt;
        &lt;value&gt;org.globus.wsrf.jndi.BeanFactory&lt;/value&gt;
        &lt;/parameter&gt;
        &lt;/resourceParams&gt;
        &lt;/resource&gt;
    </screen>
    
    <para>To configure the automatic registration of WS GRAM to the default WS MDS Index Service, change the value of the parameter 
        <computeroutput>&lt;reg&gt;</computeroutput> as follows:</para>
    <itemizedlist>
        <listitem><simpara><computeroutput>true</computeroutput> turns on auto-registration; this is the default in GT 4.0.1.</simpara></listitem>
        <listitem><simpara><computeroutput>false</computeroutput> turns off auto-registration; this is the default in GT 4.0.0.</simpara></listitem>
    </itemizedlist>
    
    <section><title>Configuring resource properties</title>
        <para>By default, the <computeroutput>GLUECE: </computeroutput> resource property (which contains GLUE data) is sent to the default Index Service:</para>
        
        <para> You can configure which resource properties are sent in WS GRAM's registration.xml file, <filename>$GLOBUS_LOCATION/etc/gram-service/registration.xml</filename>.  
            The following is the relevant section of the file (as it is set by default):</para>
        
        <screen>
            &lt;Content xsi:type="agg:AggregatorContent"
            xmlns:agg="http://mds.globus.org/aggregator/types"&gt;
            
            &lt;agg:AggregatorConfig xsi:type="agg:AggregatorConfig"&gt;
            
            &lt;agg:GetResourcePropertyPollType
                xmlns:glue="http://mds.globus.org/glue/ce/1.1"&gt;
            &lt;!-- Specifies that the index should refresh information
            every 60000 milliseconds (once per minute) --&gt;
            &lt;agg:PollIntervalMillis>60000&lt;/agg:PollIntervalMillis&gt;
            
            &lt;!-- specifies the resource property that should be
            aggregated, which in this case is the GLUE cluster
            and scheduler information RP --&gt;
            
            &lt;agg:ResourcePropertyName&gt;glue:GLUECE&lt;/agg:ResourcePropertyName&gt;
            
            &lt;/agg:GetResourcePropertyPollType&gt;
            &lt;/agg:AggregatorConfig&gt; 
            &lt;agg:AggregatorData/&gt;
            &lt;/Content&gt;
        </screen>

    </section>

</section>
<section id="s-wsgram-Interface_Config_Frag-registering-remotely">
    <title>Registering WS GRAM manually with default WS MDS Index Service</title>
    <para>If a third party needs to register an WS GRAM service manually, see
          <ulink url="http://www.globus.org/toolkit/docs/4.0/info/aggregator/re01.html#mds-servicegroup-add-registering">
	  Registering with mds-servicegroup-add</ulink>
          in the WS MDS Aggregator Framework documentation.
     </para>
</section>


<section id="s-wsgram-admin-softenv"><title>Configuring support for SoftEnv</title>

   <para>
      <emphasis role="strong">Note</emphasis>: This feature is only available beginning from
      version 4.0.5 of the toolkit.
   </para>
    
    <section id="s-wsgram-admin-softenv-overview"><title>Overview</title>
      <para>
        SoftEnv is a system designed to make it easier for users to define what
        applications they want to use, and easier for administrators to make
        applications available to users. SoftEnv has evolved from the original
        implementation called Soft designed at Northeastern University in 1994.
      </para>
      <para>
        In some environments like TeraGrid it's desirable to make use of SoftEnv
        before a job is submitted to leverage the use of an exactely
        defined software environment the job will run in.
      </para>
    </section>  
    
    <section id="s-wsgram-admin-softenv-configure"><title>Configuration</title>
      <para>
        Because this feature is very specific and may not be available on many
        systems, support for SoftEnv is disabled by default in normal job
        submissions. There is a parameter in the JNDI configuration of WS GRAM
        to enable SoftEnv support in job submissions.
      </para>
      <para>
        SoftEnv support must be enabled on a per-scheduler basis because the internal
        mechanisms to support SoftEnv vary between the different types of
        schedulers. Currently only the scheduler Fork, PBS and LSF can be configured
        to have SoftEnv support enabled, Condor not yet. To enable this feature the
        parameter 'enableDefaultSoftwareEnvironment' in the scheduler specific
        JNDI configuration must be set to 'true'. For example to enable SoftEnv support
        in the Fork scheduler, set the 'enableDefaultSoftwareEnvironment' in
        $GLOBUS_LOCATION/etc/gram-service-Fork/jndi-config.xml to 'true'.      
      </para>
      <para>
        Enabled SoftEnv support means that a users default environment will be
        created from his <computeroutput>.soft</computeroutput> file before each
        job submission automatically. The user doesn't need to provide extra SoftEnv
	keys in the extensions element of a job description. This is not done if the
	SoftEnv feature is disabled.
      </para>
      <para>
        For more information and examples, please look in the 
        <ulink url="user-index.html#s-wsgram-user-softenv">SoftEnv section of the
        user guide</ulink>.
      </para>
   
    </section>

    <section id="s-wsgram-admin-softenv-dependency"><title>Dependencies</title>
      <para>
        For the scheduler Fork SoftEnv needs to be installed on the host the
        container is running on. For PBS and LSF SoftEnv needs to be installed on
        the hosts where the jobs are executed.
      </para>
    </section>

</section>

<section id="s-wsgram-admin-substitution-variables">

    <title>Job Description Document Substitution Variables</title>
    <para>
       By default only four variables can be used in the job description document
       which are resolved to values in the service.
       These are
       <itemizedlist>
            <listitem><para>GLOBUS_USER_HOME</para></listitem>
            <listitem><para>GLOBUS_USER_NAME</para></listitem>
            <listitem><para>GLOBUS_SCRATCH_DIR</para></listitem>
            <listitem><para>GLOBUS_LOCATION</para></listitem>
       </itemizedlist>
     </para>
     
     <section id="s-wsgram-admin-substitution-variables-change">
     
       <title>Changes in WS GRAM beginning from GT version 4.0.5</title>
       <para>   
         To enable communities to define their own system-wide variables and
         enable their users to use them in their job descriptions, a new
         generic variable/value config file was added where these variables can
         be defined. If a job description document contains one of these variables
         that file will be used to resolve any matching variables.
       </para>
       <para>
         A new service parameter in the JNDI container registry defines the path
         to the variable mapping file. The mapping is done for each scheduler.
         This file is checked periodically (configurable frequency) to see if it
         has changed. If so, it is reread and the new content replaces the old.  
       </para>
       <para>
         For example for the scheduler Fork there are the following entries in
         <computeroutput>
            $GLOBUS_LOCATION/etc/gram-service-Fork/jndi-config.xml
         </computeroutput>
         which can be configured to determine the location and the refresh period
         of the variable mapping file:
       </para>
     <screen>&lt;parameter&gt;
  &lt;name&gt;
    substitutionDefinitionsFile
  &lt;/name&gt;
  &lt;value&gt;
    /root/vdt-stuff/globus/etc/gram-service-Condor/substitution definition.properties
  &lt;/value&gt;
&lt;/parameter&gt;
&lt;parameter&gt;
  &lt;name&gt;
    substitutionDefinitionsRefreshPeriod
  &lt;/name&gt;
  &lt;value&gt;
    &lt;!-- MINUTES --&gt;
    480
  &lt;/value&gt;
&lt;/parameter&gt;</screen>
       <para>
         The use of variables in the job description document that are
         <emphasis>not</emphasis> defined in the variable mapping file leads to
         the following error during job submission:
        <computeroutput>
          'No value found for RSL substitution variable &lt;variableName&gt;'
        </computeroutput>
      </para>
    </section>  
    
    <!--
    <section>
      <title>Open Questions</title>
      <para>
        <itemizedlist>
          <listitem>
            <para>
              Why is the variable mapping file on a per-scheduler basis and not
              the same for all schedulers?
            </para>
          </listitem>
          <listitem>
            <para>  
              There's no way to get a list of all defined variables yet, right?
            </para>
          </listitem>
        </itemizedlist>
      </para>    
    </section>
    -->
    
</section>

<section id="s-wsgram-Interface_Config_Frag-audit-logging"><title>Audit Logging</title>

   <para>
      <emphasis role="strong">Note</emphasis>: This feature is only available beginning from
      version 4.0.5 of the toolkit.
   </para>
   
   <section id="s-wsgram-Interfact_Config_Frag-audit-logging-overview"><title>Overview</title>
 <!--
    <para>
	  For an introduction and overview about audit logging look
      <ulink url="http://www.globus.org/solutions/">here</ulink>. Audit logging in WS-GRAM is
	  done 3 times in a job's lifecycle: When the processing starts, when the job is submitted
	  to the local resource manager and when it's fully processed or when it fails.
    </para>
-->
	<para>
      WS-GRAM provides mechanisms to provide access to audit and
      accounting information associated with jobs that are submitted to local
      resource manager (LRM) like PBS, LSF, Condor by WS-GRAM.
      GRAM is not a local resource manager but rather a protocol engine for
      communicating with a range of different local resource managers using a
      standard message format.
      In some scenarios it is desirable to get an overview over the usage of the
      underlying LRM like
      <itemizedlist>
         <listitem><para>What kind of jobs had been submitted via GRAM?</para></listitem>
         <listitem><para>How long did the processing of a job take?</para></listitem>
         <listitem><para>How many jobs had been submitted by user X?</para></listitem>
      </itemizedlist>
      The following three usecases give a better overview about the meaning and
      purpose of auditing and accounting:
      <orderedlist>
         <listitem><para>
           <emphasis role="bold">Group Access</emphasis>.
           A grid resource provider allows a remoteservice (e.g.,
           a gateway or portal) to submit jobs on behalf of multiple users. The
           grid resource provider only obtains information about the identity of
           the remote submitting service and thus does not know the identity of
           the users for which the grid jobs are submitted. This group access is
           allowed under the condition that the remote service store audit
           information so that, if and when needed, the grid resource provider
           can request and obtain information to track a specific job back to an
           individual user.
         </para></listitem>
         <listitem><para>
           <emphasis role="bold">Query Job Accounting</emphasis>.
           A client that submits a job needs to be able to
           obtain, after the job has completed, information about the resources
           consumed by that job. In portal and gateway environments where many
           users submit many jobs against a single allocation, this per-job
           accounting information is needed soon after the job completes so that
           client-side accounting can be updated. Accounting information is
           sensitive and thus should only be released to authorized parties.
         </para></listitem>
         <listitem><para>
           <emphasis role="bold">Auditing</emphasis>.
           In a distributed multi-site environment, it can be
           necessary to investigate various forms of suspected intrusion and
           abuse. In such cases, we may need to access an audit trail of the
           actions performed by a service. When accessing this audit trail, it
           will frequently be important to be able to relate specific actions
           to the user.
         </para></listitem>
      </orderedlist>
      </para>
      <para>
        The audit record of each job is stored in a DBMS and contains
      <itemizedlist>
        <listitem><para>
          <emphasis>job_grid_id</emphasis>:
          String representation of the resource EPR
        </para></listitem>
        <listitem><para>
          <emphasis>local_job_id</emphasis>:
          Job/process id generated by the scheduler
        </para></listitem>
        <listitem><para>
          <emphasis>subject_name</emphasis>:
          Distinguished name (DN) of the user
        </para></listitem>
        <listitem><para>
          <emphasis>username</emphasis>:
          Local username
        </para></listitem>
        <listitem><para>
          <emphasis>idempotence_id</emphasis>:
          Job id generated on the client-side
        </para></listitem>
        <listitem><para>
          <emphasis>creation_time</emphasis>:
          Date when the job resource is created
        </para></listitem>
        <listitem><para>
          <emphasis>queued_time</emphasis>:
          Date when the job is submitted to the scheduler
        </para></listitem>
        <listitem><para>
          <emphasis>stage_in_grid_id</emphasis>:
          String representation of the stageIn-EPR (RFT)
        </para></listitem>
        <listitem><para>
          <emphasis>stage_out_grid_id</emphasis>:
          String representation of the stageOut-EPR (RFT)
        </para></listitem>
        <listitem><para>
          <emphasis>clean_up_grid_id</emphasis>:
          String representation of the cleanUp-EPR (RFT)
        </para></listitem>
        <listitem><para>
          <emphasis>globus_toolkit_version</emphasis>:
          Version of the server-side GT
        </para></listitem>
        <listitem><para>
          <emphasis>resource_manager_type</emphasis>:
          Type of the resource manager (Fork, Condor, ...)
        </para></listitem>
        <listitem><para>
          <emphasis>job_description</emphasis>:
          Complete job description document
        </para></listitem>
        <listitem><para>
          <emphasis>success_flag</emphasis>:
          Flag that shows whether the job failed or finished successfully
        </para></listitem>
      </itemizedlist>
      </para>
      <para>
        While audit and accounting records may be generated and stored by
        different entities in different contexts, we assume here that audit
        records are generated by the GRAM service itself and accounting records
        by the LRM to which the GRAM service submits jobs. Accounting records
        could contain all information about the duration and the resource-usage
        of a job. Audit records are stored in a database indexed by a Grid job
        identifier (GJID), while accounting records are maintained by the LRM
        indexed by a local job identifier (JID).
      </para>
      <para>
        <emphasis role="bold">GRAM Service GJID creation</emphasis>
      </para>
      <para>  
        The WS-GRAM service returns an EPR that is used to control the job. The
        EPR is an XML document and cannot effectively be used as a primary key
        for a database table.  It needs to be converted from an EPR to an
        acceptable GJID format.  A utility class EPRUtil.java is included GT
        releases beginning from version 4.0.5 and can be used by both the GRAM
        service before storing the audit record and the GRAM client before
        getting audit information from the audit database.
      </para>
      <para>
        To connect the two sets of records, both audit and accounting records,
        we require that GRAM records theJID in each audit record that it
        generates. It is then straightforward for  an audit service to respond
        to requests like 'give me the charge of the job with JID x'  by first
        selecting matching record(s) from the audit table and then using the
        local JID(s) to join to the accounting table of the LRM to access
        relevant accounting record(s).
      </para>
      <para>
        We propose a Web Service interface for accessing audit and accounting
        information.  OGSA-DAI is a WSRF service that can create a single
        virtual database from two or more remote databases. 
        In the future, other per-job information like job performance data
        could be stored using the GJID or local JID as an index, and then made
        available in the same virtual database.
        The rest of this chapter focuses on how to configure WS-GRAM to enable
        Audit-Logging. A case study for TeraGrid can be read
        <ulink url="http://www.teragridforum.org/mediawiki/index.php?title=GRAM4_Audit">
          here</ulink>
      </para>
      <para>
        OGSA-DAI is available here:
        <ulink url="http://www.globus.org/toolkit/docs/4.0/techpreview/ogsadai/">
        http://www.globus.org/toolkit/docs/4.0/techpreview/ogsadai/</ulink>
      </para>
	  <para>
		Audit logging in WS-GRAM is done 3 times in a job's lifecycle: When the
		processing starts, when the job is submitted to the local resource manager
		and when it's fully processed or when it fails.
	  </para>
      <para>
        More information about how to use this data to get e.g. accounting
        information of a job, how to query that audit database for information
        via a Web Services interface etc. please go 
        <ulink url="http://www.teragridforum.org/mediawiki/index.php?title=GRAM4_Audit">
        here</ulink>
      </para>

      
      </section>
  
      <section>
        <title>Configuration</title>
        <para>
          Add the following lines to the Log4j configuration in
          <computeroutput>$GLOBUS_LOCATION/etc/container.log4j.properties
          </computeroutput> to enable audit logging:
        </para>
        <screen># GRAM AUDIT
log4j.category.org.globus.exec.service.exec.StateMachine.audit=DEBUG, AUDIT
log4j.appender.AUDIT=org.globus.exec.utils.audit.AuditDatabaseAppender
log4j.appender.AUDIT.layout=org.apache.log4j.PatternLayout
log4j.additivity.org.globus.exec.service.exec.StateMachine.audit=false</screen>
        <para>
          Add or modify the database configuration where the audit records are
          stored in <computeroutput>$GLOBUS_LOCATION/etc/gram-service/jndi-config.xml
          </computeroutput>:
        </para>
        <screen>&lt;resource name="auditDatabaseConfiguration" type="org.globus.exec.service.utils.AuditDatabaseConfiguration"&gt;
  &lt;resourceParams&gt;
    &lt;parameter&gt;
      &lt;name&gt;factory&lt;/name&gt;
      &lt;value&gt;org.globus.wsrf.jndi.BeanFactory&lt;/value&gt;
    &lt;/parameter&gt;
    &lt;parameter&gt;
      &lt;name&gt;driverClass&lt;/name&gt;
      &lt;value&gt;&lt;driverName like com.mysql.jdbc.Driver&gt;&lt;/value&gt;
    &lt;/parameter&gt;
    &lt;parameter&gt;
      &lt;name&gt;url&lt;/name&gt;
      &lt;value&gt;jdbc:mysql://&lt;host&gt;[:port]/auditDatabase&lt;/value&gt;
    &lt;/parameter&gt;
    &lt;parameter&gt;
      &lt;name&gt;user&lt;/name&gt;
      &lt;value&gt;myUsername&lt;/value&gt;
    &lt;/parameter&gt;
    &lt;parameter&gt;
      &lt;name&gt;password&lt;/name&gt;
      &lt;value&gt;myPassword&lt;/value&gt;
    &lt;/parameter&gt;
    &lt;parameter&gt;
      &lt;name&gt;globusVersion&lt;/name&gt;
      &lt;value&gt;4.0.3&lt;/value&gt;
    &lt;/parameter&gt;
  &lt;/resourceParams&gt;
&lt;/resource&gt;</screen>

      </section>
      
      <section id="s-gram-admin-auditing-dependencies"><title>Dependencies</title>
       <para>
        <emphasis role="strong">Database</emphasis>
       </para>
       <para>
        Currently database schemas for PostgreSQL and MySQL are provided to
        create the audit database table.
       </para>
      </section>
      
</section>

<!--
<section id="s-wsgram-Interface_Config_Frag-database-persistency">
    <title>
       Database Persistency
    </title>
    <para>

    </para>
</section>    
-->
