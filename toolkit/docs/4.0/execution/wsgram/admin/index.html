<?php 
$title="GT 4.0 WS GRAM : System Administrator's Guide";
include_once( "/mcs/www-unix.globus.org/include/globus_header.inc" ); ?>

<?php
include_once( "http://www-unix.globus.org/toolkit/docs/development/4.0-drafts/draft.inc" );
?>


<h1><?php echo $title; ?></h1>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li> <a href="#installing">Building and Installing</a></li>
  <li><a href="#configuring">Configuring</a></li>
  <li><a href="#testing">Testing</a></li>
  <li><a href="#security_considerations">Security Considerations </a></li>
  <li><a href="#troubleshooting">Troubleshooting</a></li>
  <li><a href="#usage">Usage statistics collection by the Globus Alliance</a></li>
</ul>
<h2><a name="introduction"></a>Introduction</h2>
<p>This guide contains advanced configuration information
    for system administrators working with WS GRAM. It provides references to
    information on procedures typically performed by system administrators, including
    installation, configuring, deploying, and testing the installation. It also
    describes additional prerequisites and host settings necessary for WS GRAM
    operation. Readers should be familiar with the <a href="../../key/">Key Concepts</a> and <a href="../../key/WS_GRAM_Approach.html">Implementation
    Approach</a> for WS GRAM to understand the motivation for and interaction
    between the various deployed components.</p>
<P>This information is in addition to the basic installation
instructions in the <a href="../../../admin/index.html">GT 4.0 System Administrator's
Guide</a>.</P>
<h2><a name="installing"></a>Building and Installing</h2>
<ul>
  <li><a href="#localprereq">Local prerequisites</a></li>
  <li> <a href="#fullinstall">Full GT 3.9.5 Installation including WS GRAM</a></li>
</ul>
<h3><a name="localprereq"></a>Local Prerequisites</h3>
<p>WS GRAM requires the following:</p>
<ul>
  <li><a href="#hostcredentials">Host credentials</a></li>
  <li><a href="#serviceaccount">GRAM service account </a></li>
  <li><a href="#gridmap">Gridmap authorization of user account</a></li>  
  <li><a href="#sudo">Functioning sudo</a></li>
  <li><a href="#localscheduler">Local scheduler </a></li>
</ul>
<h4><a name="hostcredentials"></a>Host credentials</h4>
<p> In order to use WS GRAM, the services running in the WSRF hosting environment
  require access to an appropriate host certificate.</p>
<h4><a name="serviceaccount"></a>GRAM service account</h4>
<p> WS GRAM requires a <strong>dedicated local account</strong> within which the WSRF hosting
  environment and GRAM services will execute. This account will often be a <code>globus</code> account
  used for all local services, but may also be specialized to only host WS GRAM.
  User jobs will run in separate accounts as specified in the <code>grid-mapfile</code> or
  associated authorization policy configuration of the host. 
</p>
  
<h4><a name="gridmap"></a>Gridmap authorization of user account</h4>
  <p>In order to authorize a user to call GRAM services, the security configuration must map 
  the Distinguished Name (DN) of the user to the name of the user in the system where the 
  GRAM services run. Here are the configuration steps:
  </p>
  <ol>
  <li>
  In order to obtain the DN, which is the subject of the user certificate, 
  run the <tt>bin/grid-cert-info</tt> command in $GLOBUS_LOCATION on
   the submission machine:
  <pre>
% bin/grid-cert-info -identity
/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe
  </pre>
  </li>
  <li>
  Create a <tt>/etc/grid-security/grid-mapfile</tt>. 
  The syntax is to have one line per user, with the distinguished name 
  followed by a whitespace and then the user account name on the GRAM machine. 
  Since the distinguished name usually contains whitespace, it is placed between quotation marks, 
  as in:
<pre>
"/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe" johndoe</pre></li>
  </ol>

<h4><a name="sudo"></a>Functioning sudo</h4>
<p> WS GRAM requires that the <code>sudo</code> command is installed and functioning
  on the service host where WS GRAM software will execute. </p>
<p> Authorization rules will need to be added to the <code>sudoers</code> file
  to allow the WS GRAM service account to execute (without a password) local
  scheduler adapters in the accounts of authorized GRAM users. This topic is
  covered in detail in the <a href="#configsudo">Configuring sudo</a>
  section.</p>
<h4><a name="localscheduler"></a>Local scheduler</h4>
<p> WS GRAM depends on a local mechanism for starting and controlling jobs. If
  the fork-based WS GRAM mode is to be used, no special software is required.
  For batch scheduling mechanisms, the local scheduler must be installed and
  configured for local job submission prior to deploying and operating WS GRAM.
  The supported batch schedulers in the GT 4.0 release are: PBS, Condor, LSF</p>
<p> RFT prerequisites include PostgreSQL to be installed and configured. The
  instructions are <a href="../../../data/rft/admin/index.html">here</a>. WS
  GRAM depends on RFT for file staging and cleanup. File staging from client
  host to compute host and visa versa. <strong>Important:</strong> Jobs requesting
  these functions will fail if RFT is not properly setup. </p>
<h3><a name="fullinstall"></a>Full GT 3.9.5 Installation including WS GRAM</h3>
<p>
Please refer to the <a href="../../../admin/#installinggt">GT 4.0 System
Administrator's Guide</a> for instructions on how to install the toolkit.</p>
<p>
If you wish to install only the WS-GRAM component and it's dependencies, do
the following instead of the final <code>make</code> step in the above
mentioned instructions:
<pre>
    globus$ make wsgram
    globus$ gpt-postinstall
</pre>
</p>
<h2><a name="configuring"></a>Configuring </h2>
<ul>
  <li><a href="#configsettings">Configuration settings</a></li>
  <li><a href="#setupservicecred">Setting up service credentials</a></li>
  <li>  <a href="#enablelocalscheduler">Enabling Local Scheduler Adapter</a></li>
  <li><a href="#configsudo">Configuring sudo</a></li>
  <li><a href="#nondefaultinstall">Extra steps for non-default installation</a></li>
</ul>
<h3><a name="configsettings"></a>Configuration settings </h3>
<?php
include_once( "./../WS_GRAM_Interface_Config_Frag.html" );
?>
<h3><a name="setupservicecred"></a>Setting up service credentials</h3>
  In a default build and install of the Globus Toolkit, the local account is configured to use host credentials at /etc/grid-service/containercert.pem and containerkey.pem.  If you already have host certs, then you can just copy them to the new name and set ownership.

<pre>
	% cd /etc/grid-security
	% cp hostcert.pem containercert.pem
	% cp hostkey.pem containerkey.pem
	% chown globus.globus container*.pem
</pre>

<p>Replace globus.globus with the user and group the container is
  installed as.</p>
<p> You should now have something like: </p>
<pre>
/etc/grid-security$ ls -l *.pem
-rw-r--r--  1 globus globus 1785 Oct 14 14:47 containercert.pem
-r--------  1 globus globus  887 Oct 14 14:47 containerkey.pem
-rw-r--r--  1 root   root   1785 Oct 14 14:42 hostcert.pem
-r--------  1 root   root    887 Sep 29 09:59 hostkey.pem
</pre>

The result is a copy of the host credentials which are accessible by the
container.

<p>
  If this is not an option, then you can configure an alternate location to point to host credentials -or- configure to use just a user proxy (personal mode).

<h3><a name="enablelocalscheduler"></a>Enabling Local Scheduler Adapter</h3>
<p>The batch scheduler interface implementations included in the release tarball
  are: PBS, Condor and LSF. To install one of the batch scheduler adapters, follow
  these steps (shown for pbs):</p>
<pre>    % cd $GLOBUS_LOCATION\gt3.9.5-all-source-installer

    % make gt4-gram-pbs

    % gpt-postinstall

</pre>
<p>Using PBS as the example, make sure the batch scheduler commands are in your
  path (qsub, qstat, pbsnodes). </p>
<p> For PBS, another setup step is required to configure the remote shell for
  rsh access: </p>

<pre>

    % cd $GLOBUS_LOCATION/setup/globus

    % ./setup-globus-job-manager-pbs --remote-shell=rsh

</pre>

<p>The last thing is to define the <a href="../WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</a> for PBS.</p>
<p>Done! You have added the PBS scheduler adapters to your GT installation.</p>
<h3><a name="configsudo"></a>Configuring sudo</h3>
<p>When the credentials of the service account and the job submitter are different
  (multi user mode), then GRAM will prepend a call to sudo to the local adapter
  callout command. <strong>Important:</strong> If sudo is not configured properly, the command
  and thus job will fail.</p>
<p>As <b>root</b>, here are the two lines to add to the /etc/sudoers file for
  each GLOBUS_LOCATION installation, where /opt/globus/GT3.9.5 should be replaced
  with the GLOBUS LOCATION for your installation:</p>
<pre># Globus GRAM entries
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT3.9.5/libexec/globus-gridmap-and-execute 
           /opt/globus/GT3.9.5/libexec/globus-job-manager-script.pl *
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT3.9.5/libexec/globus-gridmap-and-execute 
           /opt/globus/GT3.9.5/libexec/globus-gram-local-proxy-tool *
      </pre>
<h3><a name="nondefaultinstall"></a>Extra steps for non-default installation</h3>
<p><font color="purple">[orienting text] </font></p>
<ul>
  <li><a href="#nondefaultservicecreds">Non-default service credentials</a></li>
  <li> <a href="#nondefaultgridftp">Non-default GridFTP server </a></li>
  <li><a href="#nondefaultcontainerport">Non-default container port</a> </li>
  <li><a href="#nondefaultgridmap">Non-default gridmap</a> </li>
  <li><a href="#nondefaultjoblimit">Non-default job resource limit</a> </li>
</ul>
<h4><a name="nondefaultservicecreds"></a>Non-default service credentials</h4>
<p><font color="purple">[orienting text] </font></p>
<ul>
  <li><a href="#altlocationhostcreds">Alternative location for host credentials</a></li>
  <li><a href="#userproxy">User proxy</a></li>
</ul>
<h5><a name="altlocationhostcreds"></a>Alternate location for host credentials</h5>
<p>If setting up host credentials in the default location of /etc/grid-security/containercert.pem and containerkey.pem is <strong>not</strong> an option for you, then you can configure an alternate location to point to host credentials.
</p>
<p>Security descriptor configuration details are <a href="../../../security/authzframe/security_descriptor.html#descFile">here</a>,
  but the quick change is to edit this file - <tt>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</tt>
  - by changing the cert and key paths to point to host credentials that the
  service account owns.</p>

<h5><a name="userproxy"></a>User proxy</h5>
  To run the container using just a user proxy, simply comment out the ContainerSecDesc
  parameter in this file <tt>$GLOBUS_LOCATION/etc/globus_wsrf_core/server-config.wsdd</tt> as
  follows:
  <pre>
    &lt;!--
        &lt;parameter 
            name="containerSecDesc" 
            value="etc/globus_wsrf_core/global_security_descriptor.xml"/&gt;
     --&gt;
      </pre>
<p>Running in personal mode (user proxy), another GRAM configuration setting
  is required. For GRAM to authorize the RFT service when performing
  staging functions, it needs to know the subject DN for verification. Here are
  the steps:</p>
<pre>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --staging-subject=
         "/DC=org/DC=doegrids/OU=People/CN=Stuart Martin 564720"
      </pre>
<p>You can get your subject DN by running this command:</p>
<pre>
	% grid-cert-info -subject
</pre>

<h4><a name="nondefaultgridftp"></a>Non-default GridFTP server </h4>
<p>By default, the GridFTP server is assumed to run as root on localhost:2811.  If this is not true for your site then change it by running this command with the proper GridFTP URL values:</p>

<pre>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --gridftp-server="gsiftp://gridftp.host.org:1234"
      </pre>
<p> Also, the GridFTP host and/or port must be updated by editing the <a href="../WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</a> config file: <code>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</code>. </p>
<h4><a name="nondefaultcontainerport"></a>Non-default container port</h4>
<p>By default, the globus services will assume 8443 is the port the Globus container
  is using. However the container can be run under a non-standard port, for example:</p>
<pre>
	% globus-start-container -p 4321
      </pre>
<p>When doing this, GRAM needs to be told the port to use to contact the RFT
  service, like so:</p>
<pre>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --staging-port="4321"
      </pre>
<h4><a name="nondefaultgridmap"></a>Non-default gridmap</h4>
<p>If you wish to specify a non-standard gridmap file in a multi-user
installation, two basic configurations need to be changed:
<ul>
    <li>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml
        <ul><li>As specified in the <a href="http://www-unix.globus.org/toolkit/docs/development/4.0-drafts/security/authzframe/security_descriptor.html#configGridmap">gridmap config</a> instructions, add a &lt;gridmap value="..."/&gt;
                element to the file appropriately.</li></ul>
    </li>
    <li>/etc/sudoers
        <ul>
            <li>Add <pre>-g /path/to/grid-mapfile</pre> as the first argument to
                all instances of the <pre>globus-gridmap-and-exec</pre> command.
            </li>
        </ul>
    </li>
</ul>
</p>
<p>
Example:<br>
<br>
<i>global_security_descriptor.xml</i><br>
    <pre>
    ...

    &lt;gridmap value="/opt/grid-mapfile"/&gt;

    ...
    </pre>
<i>sudoers</i>
    <pre>
   ...

   # Globus GRAM entries
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT3.9.5/libexec/globus-gridmap-and-execute 
           -g /opt/grid-mapfile
           /opt/globus/GT3.9.5/libexec/globus-job-manager-script.pl *
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT3.9.5/libexec/globus-gridmap-and-execute 
           -g /opt/grid-mapfile
           /opt/globus/GT3.9.5/libexec/globus-gram-local-proxy-tool *

    ...
    </pre>
<h4><a name="nondefaultjoblimit"></a>Non-default job resource limit</h4>
<p>The current limit on the number of job resources (both exec and multi)
allowed to exist at any one time is 1000.  This limit was chosen from
scalability tests as an appropriate precaution to avoid out-of-memory errors.
To change this value to, say, 150, use the setup-gram-service-common script as follows:</p>
<pre>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --max-job-limit="150"
</pre>
<h2><a name="testing"></a>Testing</h2>
<p>See the WS GRAM <a href="../user/commandline_java_managed_globus_run.html#interactivemode">users
    guide</a> for information about submitting a test job.</p>
<h2><a name="security_considerations"></a>Security Considerations </h2>
<?php
include_once( "./../WS_GRAM_Security_Considerations_Frag.html" );
?>
<h2><a name="troubleshooting"></a>Troubleshooting</h2>
<?php
include_once( "./troubleshooting.html" );
?>
<h2><a name="usage"></a>Usage statistics collection by the Globus Alliance</h2>
<p>
  <?php
include_once( "./../WS_GRAM_Usage_Statistics_Frag.html" );
?>
</p>

<?php include("/mcs/www-unix.globus.org/include/globus_footer.inc"); ?>
