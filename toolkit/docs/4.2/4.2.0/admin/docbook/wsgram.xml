<chapter id="c-wsgram"> 
	<title>WS GRAM Configuration</title>
<section id="s-wsgram-introduction"><title>Introduction</title>
<para>This guide contains advanced configuration information
    for system administrators working with WS GRAM. It provides references to
    information on procedures typically performed by system administrators, including
    installation, configuring, deploying, and testing the installation. It also
    describes additional prerequisites and host settings necessary for WS GRAM
    operation. Readers should be familiar with the <ulink url="../../key/">Key Concepts</ulink> and <ulink url="../../key/WS_GRAM_Approach.html">Implementation
    Approach</ulink> for WS GRAM to understand the motivation for and interaction
    between the various deployed components.</para>
<para>This is a partially-complete docbook translation of
        <ulink url="../../execution/wsgram/admin/index.html">the WS GRAM Admin Guide</ulink>.
        Please see that document for additional information.
</para>

</section>
<section id="localprereq"><title>Local Prerequisites</title>
<para>WS GRAM requires the following:</para>
<itemizedlist>
  <listitem><simpara><link linkend="hostcredentials">Host credentials</link></simpara></listitem>
  <listitem><simpara><link linkend="serviceaccount">GRAM service account </link></simpara></listitem>
  <listitem><simpara><link linkend="gridmap">Gridmap authorization of user account</link></simpara></listitem>  
  <listitem><simpara><link linkend="sudo">Functioning sudo</link></simpara></listitem>
  <listitem><simpara><link linkend="localscheduler">Local scheduler </link></simpara></listitem>
</itemizedlist>
<section id="hostcredentials"><title>Host credentials</title>
<para> In order to use WS GRAM, the services running in the WSRF hosting environment
  require access to an appropriate host certificate.</para>
</section>
<section id="serviceaccount"><title>GRAM service account</title>
<para> WS GRAM requires a <emphasis>dedicated local account</emphasis> within which the WSRF hosting
  environment and GRAM services will execute. This account will often be a globus> account
  used for all local services, but may also be specialized to only host WS GRAM.
  User jobs will run in separate accounts as specified in the <filename>grid-mapfile</filename> or
  associated authorization policy configuration of the host. 
</para>
</section>
<section id="gridmap"><title>Gridmap authorization of user account</title>
  <para>In order to authorize a user to call GRAM services, the security configuration must map 
  the Distinguished Name (DN) of the user to the name of the user in the system where the 
  GRAM services run. Here are the configuration steps:
  </para>
  <orderedlist>
  <listitem><para>
  In order to obtain the DN, which is the subject of the user certificate, 
  run the <command>bin/grid-cert-info</command> command in $GLOBUS_LOCATION on
   the submission machine:
<screen>
% bin/grid-cert-info -identity
/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe</screen>
  </para></listitem>
  <listitem><para>
  Create a <filename>/etc/grid-security/grid-mapfile</filename>. 
  The syntax is to have one line per user, with the distinguished name 
  followed by a whitespace and then the user account name on the GRAM machine. 
  Since the distinguished name usually contains whitespace, it is placed between quotation marks, 
  as in:
<screen>
"/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe" johndoe</screen>
</para></listitem>
  </orderedlist>

</section>
<section id="sudo"><title>Functioning sudo</title>
<para> WS GRAM requires that the <command>sudo</command> command is installed and functioning
  on the service host where WS GRAM software will execute. </para>
<para> Authorization rules will need to be added to the <filename>sudoers</filename> file
  to allow the WS GRAM service account to execute (without a password) local
  scheduler adapters in the accounts of authorized GRAM users. See <link linkend="configsudo">Configuring sudo below</link></para>
</section>
<section id="localscheduler"><title>Local scheduler</title>
<para> WS GRAM depends on a local mechanism for starting and controlling jobs. If
  the fork-based WS GRAM mode is to be used, no special software is required.
  For batch scheduling mechanisms, the local scheduler must be installed and
  configured for local job submission prior to deploying and operating WS GRAM.
  The supported batch schedulers in the GT 4.0 release are: PBS, Condor, LSF</para>
</section>
<section id="rftdependency"><title>RFT Dependency</title>
<para> RFT prerequisites include PostgreSQL to be installed and configured. The
  instructions are <xref linkend="s-rft-configuring">here</xref>. WS
  GRAM depends on RFT for file staging and cleanup. File staging from client
  host to compute host and visa versa.</para>
<important><simpara>Jobs requesting
	these functions will fail if RFT is not properly setup.</simpara></important>
</section>
</section>
<section id="configuring"><title>Configuring </title>
<itemizedlist>
  <listitem><simpara><link linkend="configsettings">Configuration settings</link></simpara></listitem>
  <listitem><simpara><link linkend="setupservicecred">Setting up service credentials</link></simpara></listitem>
  <listitem><simpara><link linkend="enablelocalscheduler">Enabling Local Scheduler Adapter</link></simpara></listitem>
  <listitem><simpara><link linkend="configsudo">Configuring sudo</link></simpara></listitem>
  <listitem><simpara><link linkend="nondefaultinstall">Extra steps for non-default installation</link></simpara></listitem>
</itemizedlist>
<section id="configsettings"><title>Configuration settings </title>

	<para>include</para>

</section>
<section id="setupservicecred"><title>Setting up service credentials</title>
<para>In a default build and install of the Globus Toolkit,
the local account is configured to use host credentials at
/etc/grid-service/containercert.pem and containerkey.pem.  If you
already have host certs, then you can just copy them to the new name
and set ownership.
</para>

<screen>
% cd /etc/grid-security
% cp hostcert.pem containercert.pem
% cp hostkey.pem containerkey.pem
% chown globus.globus container*.pem</screen>

<para>Replace globus.globus with the user and group the container is
  installed as.</para>
<para> You should now have something like: </para>
<screen>
/etc/grid-security$ ls -l *.pem
-rw-r--r--  1 globus globus 1785 Oct 14 14:47 containercert.pem
-r--------  1 globus globus  887 Oct 14 14:47 containerkey.pem
-rw-r--r--  1 root   root   1785 Oct 14 14:42 hostcert.pem
-r--------  1 root   root    887 Sep 29 09:59 hostkey.pem</screen>

<para>The result is a copy of the host credentials which are accessible by the
	container.</para>

<para>
If this is not an option, then you can configure an alternate location
to point to host credentials -or- configure to use just a user proxy
(personal mode).
</para>
</section>
<section id="enablelocalscheduler"><title>Enabling Local Scheduler Adapter</title>
<para>The batch scheduler interface implementations included in the release tarball
  are: PBS, Condor and LSF. To install one of the batch scheduler adapters, follow
  these steps (shown for pbs):</para>
<screen>% cd $GLOBUS_LOCATION\gt3.9.4-all-source-installer
% make gt4-gram-pbs postinstall
% gpt-postinstall</screen>
<para>Using PBS as the example, make sure the batch scheduler commands are in your
  path (qsub, qstat, pbsnodes). </para>
<para> For PBS, another setup step is required to configure the remote shell for
  rsh access: </para>

<screen>% cd $GLOBUS_LOCATION/setup/globus
% ./setup-globus-job-manager-pbs --remote-shell=rsh</screen>

<para>The last thing is to define the <ulink url="../WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</ulink> for PBS.</para>
<para>Done! You have added the PBS scheduler adapters to your GT installation.</para>
</section>
<section id="configsudo"><title>Configuring sudo</title>
<para>When the credentials of the service account and the job submitter are different
  (multi user mode), then GRAM will prepend a call to sudo to the local adapter
  callout command.</para>
<important><simpara>If sudo is not configured properly, the command
	and thus job will fail.</simpara></important>
<para>As <emphasis>root</emphasis>, here are the two lines to add to the /etc/sudoers file for
  each GLOBUS_LOCATION installation, where /opt/globus/GT3.9.4 should be replaced
  with the GLOBUS LOCATION for your installation:</para>
<screen># Globus GRAM entries
globus  ALL=(<replaceable>username1,username2</replaceable>) 
        NOPASSWD: /opt/globus/GT3.9.4/libexec/globus-gridmap-and-execute 
        /opt/globus/GT3.9.4/libexec/globus-job-manager-script.pl *
globus  ALL=(<replaceable>username1,username2</replaceable>) 
        NOPASSWD: /opt/globus/GT3.9.4/libexec/globus-gridmap-and-execute 
        /opt/globus/GT3.9.4/libexec/globus-gram-local-proxy-tool *</screen>
</section>
<section id="nondefaultinstall"><title>Extra steps for non-default installation</title>
<itemizedlist>
  <listitem><simpara><link linkend="nondefaultservicecreds">Non-default service credentials</link></simpara></listitem>
  <listitem><simpara><link linkend="nondefaultgridftp">Non-default GridFTP server </link></simpara></listitem>
  <listitem><simpara><link linkend="nondefaultcontainerport">Non-default container port</link> </simpara></listitem>
  <listitem><simpara><link linkend="nondefaultgridmap">Non-default gridmap</link> </simpara></listitem>
  <listitem><simpara><link linkend="nondefaultjoblimit">Non-default job resource limit</link> </simpara></listitem>
</itemizedlist>
<section id="nondefaultservicecreds"><title>Non-default service credentials</title>
<itemizedlist>
	<listitem><simpara><link linkend="altlocationhostcreds">Alternative location for host credentials</link></simpara></listitem>
  <listitem><simpara><link linkend="userproxy">User proxy</link></simpara></listitem>
</itemizedlist>
</section>
<section id="altlocationhostcreds"><title>Alternate location for host credentials</title>
<para>If setting up host credentials in the default location of /etc/grid-security/containercert.pem and containerkey.pem is <emphasis>not</emphasis> an option for you, then you can configure an alternate location to point to host credentials.
</para>
<para>Security descriptor configuration details are <ulink url="../../../security/authzframe/security_descriptor.html#descFile">here</ulink>,
  but the quick change is to edit this file - <filename>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</filename>
  - by changing the cert and key paths to point to host credentials that the
  service account owns.</para>

</section>
<section id="userproxy"><title>User proxy</title>
	<para>To run the container using just a user proxy, simply comment out the ContainerSecDesc
  parameter in this file <filename>$GLOBUS_LOCATION/etc/globus_wsrf_core/server-config.wsdd</filename> as
  follows:</para>
  <screen>
&lt;!--
    &lt;parameter 
        name="containerSecDesc" 
        value="etc/globus_wsrf_core/global_security_descriptor.xml"/&gt;
 --&gt;
  </screen>
<para>Running in personal mode (user proxy), another GRAM configuration setting
  is required. For GRAM to authorize the RFT service when performing
  staging functions, it needs to know the subject DN for verification. Here are
  the steps:</para>
<screen>
% cd $GLOBUS_LOCATION/setup/globus
% ./setup-gram-service-common --staging-subject=
 "/DC=org/DC=doegrids/OU=People/CN=Stuart Martin 564720"</screen>
<para>You can get your subject DN by running this command:</para>
<screen>
% grid-cert-info -subject</screen>
</section>
<section id="nondefaultgridftp"><title>Non-default GridFTP server </title>
<para>By default, the GridFTP server is assumed to run as root on localhost:2811.  If this is not true for your site then you must updated the configuration by editing the <ulink url="../WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</ulink> config file: <filename>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</filename>. </para>
</section>
<section id="nondefaultcontainerport"><title>Non-default container port</title>
<para>By default, the globus services will assume 8443 is the port the Globus container
  is using. However the container can be run under a non-standard port, for example:</para>
<screen>
% globus-start-container -p 4321</screen>
<para>When doing this, GRAM needs to be told the port to use to contact the RFT
  service, like so:</para>
<screen>
% cd $GLOBUS_LOCATION/setup/globus
% ./setup-gram-service-common --staging-port="4321"</screen>
</section>
<section id="nondefaultgridmap"><title>Non-default gridmap</title>
<para>If you wish to specify a non-standard gridmap file in a multi-user
installation, two basic configurations need to be changed:
<itemizedlist>
	<listitem><para><filename>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</filename>
</para><para>
As specified in the <ulink url="http://www-unix.globus.org/toolkit/docs/development/4.0-drafts/security/authzframe/security_descriptor.html#configGridmap">gridmap config</ulink> instructions, add a &lt;gridmap value="..."/&gt;
element to the file appropriately.</para>
    </listitem>
    <listitem><para><filename>/etc/sudoers</filename></para>
	    <para>Add "-g /path/to/grid-mapfile" as the first argument to
                all instances of the <command>globus-gridmap-and-exec</command> command.
            </para>
    </listitem>
</itemizedlist>
</para>
<para>
Example:

<emphasis>global_security_descriptor.xml</emphasis>
<screen>
...

&lt;gridmap value="/opt/grid-mapfile"/&gt;

...</screen>
<emphasis>sudoers</emphasis>
<screen>
...

# Globus GRAM entries
globus  ALL=(username1,username2) 
        NOPASSWD: /opt/globus/GT3.9.4/libexec/globus-gridmap-and-execute 
        -g /opt/grid-mapfile
        /opt/globus/GT3.9.4/libexec/globus-job-manager-script.pl *
globus  ALL=(username1,username2) 
        NOPASSWD: /opt/globus/GT3.9.4/libexec/globus-gridmap-and-execute 
        -g /opt/grid-mapfile
        /opt/globus/GT3.9.4/libexec/globus-gram-local-proxy-tool *

...</screen>
</para>
</section>
<section id="nondefaultjoblimit"><title>Non-default job resource limit</title>
<para>The current limit on the number of job resources (both exec and multi)
allowed to exist at any one time is 1000.  This limit was chosen from
scalability tests as an appropriate precaution to avoid out-of-memory errors.
To change this value to, say, 150, use the setup-gram-service-common script as follows:</para>
<screen>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --max-job-limit="150"
</screen>
</section>
</section>
</section>
<section id="testing"><title>Testing</title>
<para>See the WS GRAM <ulink url="../user/commandline_java_managed_globus_run.html#interactivemode">users
    guide</ulink> for information about submitting a test job.</para>
</section>
<section id="security_considerations"><title>Security Considerations </title>

	<para>import</para>

</section>
<section id="troubleshooting"><title>Troubleshooting</title>
<para>[todo]</para>
</section>
</chapter>

