<title>GT 4.0 WS GRAM : System Administrator's Guide</title>
<titleabbrev> System Administrator's Guide</titleabbrev>
 

<section id="s-wsgram-admin-introduction"><title>Introduction</title>
<para>This guide contains advanced configuration information
    for system administrators working with WS GRAM. It provides references to
    information on procedures typically performed by system administrators, including
    installation, configuring, deploying, and testing the installation. It also
    describes additional prerequisites and host settings necessary for WS GRAM
    operation. Readers should be familiar with the <ulink url="../../key/">Key Concepts</ulink> and <ulink url="../../key/WS_GRAM_Approach.html">Implementation
    Approach</ulink> for WS GRAM to understand the motivation for and interaction
    between the various deployed components.</para>
<para>This information is in addition to the basic installation
instructions in the <ulink url="../../admin/index.html">GT 4.0 System Administrator's
Guide</ulink>.</para>
</section>


<section id="s-wsgram-admin-installing"><title>Building and Installing</title>

<section id="s-wsgram-admin-localprereq"><title>Local Prerequisites</title>
<para>WS GRAM requires the following:</para>


<section id="s-wsgram-admin-hostcredentials"><title>Host credentials</title>
<para> In order to use WS GRAM, the services running in the WSRF hosting environment
  require access to an appropriate host certificate.</para>
</section>

<section id="s-wsgram-admin-serviceaccount"><title>GRAM service account</title>
<para> WS GRAM requires a <emphasis>dedicated local account</emphasis> within which the WSRF hosting
  environment and GRAM services will execute. This account will often be a <computeroutput>globus</computeroutput> account
  used for all local services, but may also be specialized to only host WS GRAM.
  User jobs will run in separate accounts as specified in the <computeroutput>grid-mapfile</computeroutput> or
  associated authorization policy configuration of the host. 
</para>
</section>

<section id="s-wsgram-admin-gridmap"><title>Gridmap authorization of user account</title>
  <para>In order to authorize a user to call GRAM services, the security configuration must map 
  the Distinguished Name (DN) of the user to the name of the user in the system where the 
  GRAM services run. Here are the configuration steps:
  </para>
  <orderedlist>
  <listitem><para>
  In order to obtain the DN, which is the subject of the user certificate, 
  run the <computeroutput>bin/grid-cert-info</computeroutput> command in $GLOBUS_LOCATION on
   the submission machine:
  <screen>
% bin/grid-cert-info -identity
/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe
  </screen>
  </para></listitem>
  <listitem><para>
  Create a <computeroutput>/etc/grid-security/grid-mapfile</computeroutput>. 
  The syntax is to have one line per user, with the distinguished name 
  followed by a whitespace and then the user account name on the GRAM machine. 
  Since the distinguished name usually contains whitespace, it is placed between quotation marks, 
  as in:
<screen>
"/O=Grid/OU=GlobusTest/OU=simpleCA-foo.bar.com/OU=bar.com/CN=John Doe" johndoe</screen></para></listitem>
  </orderedlist>

</section>
<section id="s-wsgram-admin-sudo"><title>Functioning sudo</title>
<para> WS GRAM requires that the <computeroutput>sudo</computeroutput> command is installed and functioning
  on the service host where WS GRAM software will execute. </para>
<para> Authorization rules will need to be added to the <computeroutput>sudoers</computeroutput> file
  to allow the WS GRAM service account to execute (without a password) local
  scheduler adapters in the accounts of authorized GRAM users. This topic is
  covered in detail in the <ulink url="#configsudo">Configuring sudo</ulink>
  section.</para>
</section>
<section id="s-wsgram-admin-localscheduler"><title>Local scheduler</title>
<para> WS GRAM depends on a local mechanism for starting and controlling jobs. If
  the fork-based WS GRAM mode is to be used, no special software is required.
  For batch scheduling mechanisms, the local scheduler must be installed and
  configured for local job submission prior to deploying and operating WS GRAM.
  The supported batch schedulers in the GT 4.0 release are: PBS, Condor, LSF</para>
<para> RFT prerequisites include PostgreSQL to be installed and configured. The
  instructions are <ulink url="../../data/rft/admin/index.html">here</ulink>. WS
  GRAM depends on RFT for file staging and cleanup. File staging from client
  host to compute host and visa versa. <emphasis>Important:</emphasis> Jobs requesting
  these functions will fail if RFT is not properly setup. </para>
</section>
</section>

<section id="s-wsgram-admin-fullinstall"><title>Full GT 4.0.0 Installation including WS GRAM</title>
<para>
Please refer to the <ulink url="../../admin/#installinggt">GT 4.0 System
Administrator's Guide</ulink> for instructions on how to install the toolkit.</para>
<para>
If you wish to install only the WS-GRAM component and it's dependencies, do
the following instead of the final <computeroutput>make</computeroutput> step in the above
mentioned instructions:
<screen>
    globus$ make wsgram
    globus$ gpt-postinstall
</screen>
</para>
</section>
</section>


<section id="s-wsgram-admin-configuring"><title>Configuring </title>

<section id="s-wsgram-admin-configsettings"><title>Configuration settings </title>
<para><xref linkend="s-wsgram-Public_Interfaces-config"/></para>
</section>

<section id="s-wsgram-admin-setupservicecred"><title>Setting up service credentials</title>
<para>  In a default build and install of the Globus Toolkit, the local account is configured to use host credentials at /etc/grid-service/containercert.pem and containerkey.pem.  If you already have host certs, then you can just copy them to the new name and set ownership.

<screen>
	% cd /etc/grid-security
	% cp hostcert.pem containercert.pem
	% cp hostkey.pem containerkey.pem
	% chown globus.globus container*.pem
</screen>
</para>
<para>Replace globus.globus with the user and group the container is
  installed as.</para>
<para> You should now have something like:
<screen>
/etc/grid-security$ ls -l *.pem
-rw-r--r--  1 globus globus 1785 Oct 14 14:47 containercert.pem
-r--------  1 globus globus  887 Oct 14 14:47 containerkey.pem
-rw-r--r--  1 root   root   1785 Oct 14 14:42 hostcert.pem
-r--------  1 root   root    887 Sep 29 09:59 hostkey.pem
</screen>

The result is a copy of the host credentials which are accessible by the
container.
</para>
<para>
  If this is not an option, then you can configure an alternate location to point to host credentials -or- configure to use just a user proxy (personal mode).
</para>
</section>

<section id="s-wsgram-admin-enablelocalscheduler"><title>Enabling Local Scheduler Adapter</title>
<para>The batch scheduler interface implementations included in the release tarball
  are: PBS, Condor and LSF. To install one of the batch scheduler adapters, follow
  these steps (shown for pbs):
<screen>    % cd $GLOBUS_LOCATION\gt4.0.0-all-source-installer

    % make gt4-gram-pbs

    % gpt-postinstall

</screen></para>
<para>Using PBS as the example, make sure the batch scheduler commands are in your
  path (qsub, qstat, pbsnodes). </para>
<para> For PBS, another setup step is required to configure the remote shell for
  rsh access: 

<screen>

    % cd $GLOBUS_LOCATION/setup/globus

    % ./setup-globus-job-manager-pbs --remote-shell=rsh

</screen>
</para>
<para>The last thing is to define the <ulink url="WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</ulink> for PBS.</para>
<para>Done! You have added the PBS scheduler adapters to your GT installation.</para>
</section>

<section id="s-wsgram-admin-configsudo"><title>Configuring sudo</title>
<para>When the credentials of the service account and the job submitter are
different (multi user mode), then GRAM will prepend a call to sudo to the local
adapter callout command. <emphasis>Important:</emphasis> If sudo is not
configured properly, the command, and consequently the job, will fail.</para>
<para>As <emphasis>root</emphasis>, here are the two lines to add to the
/etc/sudoers file for each installation, where
/opt/globus/GT4.0.0 should be replaced with the GLOBUS LOCATION for your
installation:
<screen># Globus GRAM entries
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute
           -g /etc/grid-security/grid-mapfile
           /opt/globus/GT4.0.0/libexec/globus-job-manager-script.pl *
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
           -g /etc/grid-security/grid-mapfile
           /opt/globus/GT4.0.0/libexec/globus-gram-local-proxy-tool *
      </screen></para>
</section>




<section id="s-wsgram-admin-nondefaultinstall"><title>Extra steps for non-default installation</title>

<section id="s-wsgram-admin-nondefaultservicecreds"><title>Non-default service credentials</title>

<section id="s-wsgram-admin-altlocationhostcreds"><title>Alternate location for host credentials</title>
<para>If setting up host credentials in the default location of /etc/grid-security/containercert.pem and containerkey.pem is <emphasis>not</emphasis> an option for you, then you can configure an alternate location to point to host credentials.
</para>
<para>Security descriptor configuration details are <ulink url="../../security/authzframe/security_descriptor.html#descFile">here</ulink>,
  but the quick change is to edit this file - <computeroutput>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</computeroutput>
  - by changing the cert and key paths to point to host credentials that the
  service account owns.</para>
</section>

<section id="s-wsgram-admin-userproxy"><title>User proxy</title>
<para>  To run the container using just a user proxy, simply comment out the ContainerSecDesc
  parameter in this file <computeroutput>$GLOBUS_LOCATION/etc/globus_wsrf_core/server-config.wsdd</computeroutput> as
  follows:
  <screen>
    &lt;!--
        &lt;parameter 
            name="containerSecDesc" 
            value="etc/globus_wsrf_core/global_security_descriptor.xml"/&gt;
     --&gt;
      </screen>
Running in personal mode (user proxy), another GRAM configuration setting
  is required. For GRAM to authorize the RFT service when performing
  staging functions, it needs to know the subject DN for verification. Here are
  the steps:
<screen>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --staging-subject=
         "/DC=org/DC=doegrids/OU=People/CN=Stuart Martin 564720"
      </screen>
You can get your subject DN by running this command:
<screen>
	% grid-cert-info -subject
</screen></para>
</section>
</section>


<section id="s-wsgram-admin-nondefaultgridftp"><title>Non-default GridFTP server </title>
<para>By default, the GridFTP server is assumed to run as root on localhost:2811.  If this is not true for your site then change it by running this command with the proper GridFTP URL values:

<screen>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --gridftp-server="gsiftp://gridftp.host.org:1234"
      </screen>
Also, the GridFTP host and/or port must be updated by editing the <ulink url="WS_GRAM_Public_Interfaces.html#filesysmap">GRAM
    and GridFTP file system mapping</ulink> config file: <computeroutput>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</computeroutput>. </para>
</section>

<section id="s-wsgram-admin-nondefaultcontainerport"><title>Non-default container port</title>
<para>By default, the globus services will assume 8443 is the port the Globus container
  is using. However the container can be run under a non-standard port, for example:
<screen>
	% globus-start-container -p 4321
      </screen>
When doing this, GRAM needs to be told the port to use to contact the RFT
  service, like so:
<screen>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --staging-port="4321"</screen>
      
</para></section>

<section id="s-wsgram-admin-nondefaultgridmap"><title>Non-default gridmap</title>
<para>If you wish to specify a non-standard gridmap file path in a multi-user
installation, two basic configurations need to be changed:</para>

<itemizedlist>
    <listitem><para>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml
        <itemizedlist><listitem><simpara>As specified in the <ulink url="http://www-unix.globus.org/toolkit/docs/development/4.0-drafts/security/authzframe/security_descriptor.html#configGridmap">gridmap config</ulink> instructions, change
        the &lt;gridmap value="..."/&gt; element to the non-standard gridmap
        file path appropriately.</simpara></listitem>
    </itemizedlist>
    </para></listitem>

    <listitem><para>/etc/sudoers
        <itemizedlist>
            <listitem><para>Change the <screen>-g</screen> option to
            both instances of <screen>globus-gridmap-and-execute</screen> to the
            path of the non-standard gridmap file.
            </para></listitem>
        </itemizedlist>
    </para></listitem>
</itemizedlist>

<para>
Example:

<emphasis>global_security_descriptor.xml</emphasis>
<screen>
    ...

    &lt;gridmap value="/opt/grid-mapfile"/&gt;

    ...
    </screen>
<emphasis>sudoers</emphasis>
    <screen>
   ...

   # Globus GRAM entries
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
           -g /opt/grid-mapfile
           /opt/globus/GT4.0.0/libexec/globus-job-manager-script.pl *
   globus  ALL=(username1,username2) 
           NOPASSWD: /opt/globus/GT4.0.0/libexec/globus-gridmap-and-execute 
           -g /opt/grid-mapfile
           /opt/globus/GT4.0.0/libexec/globus-gram-local-proxy-tool *

    ...
    </screen>
</para>
<para>If 
</section>
<section id="s-wsgram-admin-nondefaultjoblimit"><title>Non-default job resource limit</title>
<para>The current limit on the number of job resources (both exec and multi)
allowed to exist at any one time is 1000.  This limit was chosen from
scalability tests as an appropriate precaution to avoid out-of-memory errors.
To change this value to, say, 150, use the setup-gram-service-common script as follows:</para>
<screen>
	% cd $GLOBUS_LOCATION/setup/globus
	% ./setup-gram-service-common --max-job-limit="150"
</screen>


</section>
</section>
</section>



<section id="s-wsgram-admin-testing"><title>Testing</title>
<para>See the WS GRAM <ulink url="../user/commandline_java_managed_globus_run.html#interactivemode">users
    guide</ulink> for information about submitting a test job.</para>
</section>


<section id="s-wsgram-admin-security_considerations"><title>Security Considerations </title>
&WS_GRAM_Security_Considerations_Frag;
</section>

<section id="s-wsgram-admin-troubleshooting"><title>Troubleshooting</title>
&WS_GRAM_Troubleshooting_Frag;
</section>

<section id="s-wsgram-admin-usage"><title>Usage statistics collection by the Globus Alliance</title>
&WS_GRAM_Usage_Statistics_Frag;
</section>





