<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">

<chapter id="gram4-troubleshooting"><title>Troubleshooting</title>
    <indexterm type="gram4"><primary>troubleshooting</primary></indexterm>
    <indexterm type="gram4"><primary>troubleshooting</primary><secondary>errors</secondary></indexterm>
     <para>For a list of common errors in GT, see <olink targetdoc="gtuser" targetptr="errors"/>.</para>
<para>For information about sys admin logging, see <olink targetdoc="gram4Admin" targetptr="gram4-admin-debugging"/> in the GRAM4 
    Admin Guide.</para>
<section><title>Troubleshooting tips</title>    
    <para>In case you run into problems you can do the following</para>
    <itemizedlist>
      <listitem><simpara><indexterm type="gram4"><primary>troubleshooting</primary><secondary>check documentation</secondary></indexterm>Check the GRAM4 documentation. Maybe you'll
        find hints here to solve your problem.</simpara>
      </listitem>
      <listitem><simpara><indexterm type="gram4"><primary>troubleshooting</primary><secondary>mailing lists</secondary></indexterm>Send e-mails to one of several Globus e-mail lists.
        You'll have to subscribe to a list before you can send an e-mail to it.
        See <ulink url="http://dev.globus.org/wiki/Mailing_Lists">here</ulink> for
        general e-mail lists and information on how to subscribe to a list and
        <ulink url="http://dev.globus.org/wiki/GRAM#Mailing_Lists">here</ulink>
        for GRAM specific lists.</simpara>
        <simpara>Probably the best lists for GRAM4-related problems are
        gt-user@globus.org and gram-user@globus.org</simpara>
      </listitem>
      <listitem><simpara><indexterm type="gram4"><primary>troubleshooting</primary><secondary>check container log</secondary></indexterm>Check the container log for errors.</simpara>
        <simpara>In case you don't find anything suspicious you can increase the
        log-level of GRAM4 or other relevant components. 
        Maybe the additional logging-information will
        tell you what's going wrong. General information about container logging can be
        found <olink targetdoc="javawscoreAdmin" targetptr="javawscore-logging">Logging
        in Java WS Core</olink> section.</simpara>
        <simpara>To get debug information from GRAM4, un-comment the following line
        in <computeroutput>$GLOBUS_LOCATION/container-log4j.properties</computeroutput>
        by removing the leading '#' and restart the GT4 server.</simpara>
        <screen># log4j.category.org.globus.exec=DEBUG</screen>
        <simpara>The logging output can either be found on the console if you started
        the container using <computeroutput>globus-start-container</computeroutput>
        (maybe with arguments) or in <computeroutput>$GLOBUS_LOCATION/var/container.log</computeroutput> in
        if you started the container using the command
        <computeroutput>globus-start-container-detached</computeroutput></simpara>
      </listitem> 
    </itemizedlist>
</section>
    
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
    href="../../common/javawscore/Java_WS_Core_Errors_Frag.xml"/>
    
        <xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
        href="GRAM4_Errors_Frag.xml"/>
    
<!--    
<section><title>The job manager detected an invalid script response</title>
    <para>[fixme - the bug referenced shows resolved and fixed - can we take this out?] Check for a restrictive umask. When the service writes the native
        scheduler <glossterm>job description</glossterm> to a file, an overly restrictive umask will
        cause the permissions on the file to be such that the submission
        script run through <glossterm baseform="superuser do">sudo</glossterm> as the user cannot read the file (bug #2655).
    </para>
</section>

    <section>
<title>Fork jobs work fine, but submitting PBS jobs with globusrun-ws hangs at: <computeroutput>Current job state: Unsubmitted</computeroutput></title>

<itemizedlist>
    <listitem><simpara>Make sure the the log_path in
    <filename>$GLOBUS_LOCATION/etc/globus-pbs.conf</filename> points to locally accessible scheduler
    logs that are readable by the user running the container.  The Scheduler
    Event Generator (SEG) will not work without local scheduler logs to monitor.
    This can also apply to other resource managers, but is most comonly seen
    with PBS.
    </simpara></listitem>
    <listitem><simpara>If the SEG configuration looks sane, try running the SEG
    tests. They are located in
    <filename>$GLOBUS_LOCATION/test/globus_scheduler_event_generator_*_test/</filename>. If Fork jobs
    work, you only need to run the PBS test. Run each test by going to the
    associated directory and run <filename>./TESTS.pl</filename>. If any tests fail, report this to
    the gram-dev@globus.org mailing list.
    </simpara></listitem>
    <listitem><simpara>If the SEG tests succeed, the next step is to figure out
    the ID assigned by PBS to the queued job. Enable GRAM debug logging by
    uncommenting the appropriate line in the
    <filename>$GLOBUS_LOCATION/container-log4j.properties</filename> configuration file.
    Restart the container, run a PBS job, and search the container log for a
    line that contains "Received local job ID" to obtain the local job ID.
    </simpara></listitem>
    <listitem><simpara>Once you have the local job ID you can check the latest
    PBS logs pointed to by the value of "log_path" in
    <filename>$GLOBUS_LOCATION/etc/globus-pbs.conf</filename> to make sure the job's status is being
    logged. If the status is not being logged, check the documentation for your
    flavor of PBS to see if there's any futher configuration that needs to be
    done to enable job status logging. For example, PBS Pro requires a
    sufficient -e &lt;bitmask&gt; option added to the pbs_server command line to
    enable enough logging to satisfy the SEG.
    </simpara></listitem>
    <listitem><simpara>If the correct status is being logged, try running the
    SEG manually to see if it is reading the log file properly. The general
    form of the SEG command line is as follows:

    <computeroutput>
    $GLOBUS_LOCATION/libexec/globus-scheduler-event-generator -s pbs -t &lt;timestamp&gt;
    </computeroutput>

    The timestamp is in seconds since the epoch and dictates how far back in the
    log history the SEG should scan for job status events. The command should
    hang after dumping some status data to stdout. If no data appears, change
    the timestamp to an earlier time. If nothing ever appears, report this to
    the gram-user@globus.org mailing list.
    </simpara></listitem>
    <listitem><simpara>If running the SEG manually succeeds, try running another
    job and make sure the job process actually finishes and PBS has logged the
    correct status before giving up and cancelling globusrun-ws. If things are
    still not working, report your problem and exactly what you have tried to
    remedy the situtation to the gram-user@globus.org mailing list.
    </simpara></listitem>
</itemizedlist>
</section>
<section><title>When restarting the container, I get the following error:
<computeroutput>Error getting delegation resource</computeroutput></title>

    <para>Most likely this is simply a case of the delegated
    credential expiring.  Either refresh it for the affected job or destroy
    the job resource.
    </para>
</section>
-->
      
</chapter>