<html>
<head>
<title>Globus Toolkit 4.0: WS_GRAM Administration</title>
</head>
<body>

<h1>Globus Toolkit 4.0: WS_GRAM Administration</h1>

<h2>Document Summary</h2>

<p>
This document describes the installation and deployment of WS_GRAM
including administrator-selected features, configuration options, and
additional host settings necessary for WS_GRAM operation.  Readers
should be familiar with the <a href="WS_GRAM_Key_Concepts.html">Key
Concepts</a> and <a href="WS_GRAM_Approach.html">Implementation
Approach</a> for WS_GRAM to understand the motivation for and
interaction of the various deployed components.
</p>

<h2>Local Prerequisites</h2>

<h3>Host credentials</h3>

<p>
In order to use WS_GRAM, the services running in the WSRF hosting
environment require access to an appropriate host certificate. <i>Link
to host credential docs here...</i>
</p>

<h3>GRAM service account</h3>

<p>
WS_GRAM requires a dedicated local account within which the WSRF
hosting environment and GRAM services will execute. This account will
often be a <code>globus</code> account used for all local services, but
may also be specialized to only host WS_GRAM.  User jobs will run in
separate accounts as specified in the <code>grid-mapfile</code> or associated
authorization policy configuration of the host.
</p>

<p>
<i>Other service account information here...</i>
</p>

<h3>Functioning sudo</h3>

<p>
WS_GRAM requires that the <code>sudo</code> command is installed and
functioning on the service host where WS_GRAM software will execute.
</p>

<p>
At the completion of WS_GRAM setup, additional authorization rules
will be added to the <code>sudoers</code> file to allow the WS_GRAM
service account to execute (without a password) local scheduler
adapters in the accounts of authorized GRAM users. This topic is
covered in detail in the Software Setup section.
</p>

<h3>Local scheduler</h3>

<p>
WS_GRAM depends on a local mechanism for starting and controlling
jobs. If the fork-based WS_GRAM mode is to be used, no special
software is required. For batch scheduling mechanisms, the local
scheduler must be installed and configured for local job submission
prior to deploying and operating WS_GRAM.
</p>

<p>
<i>Other local scheduler information here...</i>
</p>

<h2>GT Dependencies</h2>

<p>
There are multiple software prerequisites for WS_GRAM. This section is
relevant if you already have installed some of these and wish to
integrate WS_GRAM into a customized deployment. A default installation
of all required GT components is described in the Software Installation
section.
</p>

<h3>WSRF Core</h3>

<p>
<i>Details of config options required for GRAM to function (if any)...</i>
</p>

<h3>GridFTP</h3>

<p>
<i>Details of config options required for GRAM to function (if any)...</i>
</p>

<h3>RFT</h3>

<p>
GRAM depends on RFT for file staging (to and from) the compute resource and file clean up.  Jobs requesting these functions will fail Without RFT properly setup.
<br><br>
RFT prerequisites include PostgreSQL to be installed and configured.  The instructions are <a href="http://www-unix.globus.org/toolkit/docs/3.2/installation/install_config_rft.html#settinguppostgresql">here</a>
<br><br>
TODO - change HERE link to RFT instructions for 3.9.3
</p>

<h3>MDS</h3>

<p>
GRAM depends on the MDS to provide discovery information about the compute resource where gram jobs can be submitted.  The discovery information is structured by the <a href="http://www.cnaf.infn.it/~sergio/datatag/glue/index.htm">GLUE schemas</a>, "the aim is to define an information model and mapping to concrete schemas for representing Grid resources."  
<br><br>
MDS prerequisites include either <a href="http://ganglia.sourceforge.net/">ganglia</a> or <a href="http://www.cs.wisc.edu/condor/hawkeye/">hawkeye</a> to be installed and configured.  These are monitoring systems used to gather the discovery information.

Detailed instructions are <a href="http://www.globus.org/mds">here</a>
<br>
TODO - change HERE link to MDS instructions for 3.9.3
<br><br>
<b>NOTE:</b> if you don't install ganglia or hawkeye, GRAM can still be used for submission and monitoring of individual jobs.  However, there will be limited monitoring/discovery information available about the compute resource.

</p>

<h3>Delegation</h3>

<p>
Included in the Globus Toolkit is the delegation factory service. GRAM depends on it is used to provide user-signed credentials to GRAM.  These credentials are used to securely perform file staging, job control, and process-specific tasks on the user's behalf.

By Default, this service assumes ownership of the host credentials in /etc/grid-services.  If that is the case for you site, then no special configuration is needed.
</p>

<h2>Software Installation</h2>

<h3>Full GT4 Installation including WS_GRAM</h3>

<p>
<i>Summarize requirements including WSRF core stuff, e.g. java</i><br>

<i>Choosing an install location</i><br>

<i>Full sequence of steps...</i>
</p>

<h3>Adding WS_GRAM to existing GT4</h3>

<p>
<i>Full sequence of steps...</i>
</p>

<h2>Software Setup</h2>

<p>
After the software packages are installed, additional steps are
required to link them together so that WS_GRAM services are started as
part of the WSRF hosting environment, WS_GRAM local adapters are
configured for use, and so that operating policies are set as desired.
</p>

<h3>Run setup tool(s)</h3>

<h3>Install host credential?</h3>

<h3>Enabling Local Scheduler Adapter</h3>

<p>
<i>Defining ManagedJobFactoryResource type</i>
</p>

<h3>Extra steps for non-default installation</h3>

<p>
<i>Pointing WS_GRAM at Delegation and RFT services</i>
</p>

<h2>Localized Settings</h2>

<h3>Local adapter callouts</h3>

<p>
<i>Using (or not) <code>gridmap_authorize_and_exec</code>...</i><br>

<i>Reference to adapter script(s)</i>
</p>

<h3>Sudo configuration</h3>

<p>
<i>Matching sudoers rules for GRAM service to make callouts to
adapter and delegation tool w/ or w/out auth_and_exec</i>
</p>

<h3>Grid-mapfile entries</h3>

<p>
<i>Need map entries for job-submitters as well as GridFTP</i>
</p>

<h3>Filesystem map</h3>

<p>
<i>Default all-shared config...</i><br>

<i>Per-volume mount descriptions</i>
</p>

</body>
</html>
