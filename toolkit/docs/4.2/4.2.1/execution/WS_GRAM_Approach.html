<html>
<head>
<title>Globus Toolkit 4.0: WS_GRAM Approach</title>
</head>
<body>

<h1>Globus Toolkit 4.0: WS_GRAM Approach</h1>

<h2>Introduction</h2>

<p>

The WS_GRAM software implements a solution to the job-management
problem described in the <a href="WS_GRAM_Key_Concepts.html">GT 4.0
WS_GRAM Key Concepts</a> document, providing Web services interfaces
consistent with the WSRF model. This solution is specific to operating
systems following the Unix programming and security model, (AS
DESCRIBED IN THE SUPPORTED PLATFORMS DOCUMENTATION?).

</p>

<h2>Component Architecture Approach</h2>

<p>
WS_GRAM combines job-management services and local system adapters
with other service components of GT4.0 in order to support job
execution with coordinated file staging.
</p>

<img src="WS_GRAM_components.png">

<h3>Globus Toolkit Components used by WS_GRAM</h3>

<p>
<dl>

<dt>ReliableFileTransfer</dt>

<dd><p>The ReliableFileTransfer (RFT) service of GT4 is invoked by the
WS_GRAM services to effect file staging before and after job
computations.</p>

<p>
The integration with RFT provides a much more robust file staging
manager than the ad-hoc solution present in previous versions of the
GRAM job manager logic. RFT has better support for retry, restart, and
fine-grained control of these capabilities. WS_GRAM exposes the full
flexibility of the RFT request language in the job staging clauses of
the job submission language.
</p>
</dd>

<dt>GridFTP</dt>

<dd><p>GridFTP servers are required to access remote storage elements
as well as filesystems accessible to the local compute elements that
may host the job. The ReliableFileTransfer Web service acts as a
so-called third-party client to the GridFTP servers in order to manage
transfers of data between remote storage elements and the compute
element filesystems. It is not necessary that GridFTP be deployed on
the same host/node as the WS_GRAM services, but staging will only be
possible to the subset of filesystems that are shared by the GridFTP
server that is registered with the WS_GRAM service. (REF TO
DEPLOY/CONFIG HERE) If no such server or shared fileystems are
registered, staging is disallowed to that WS_GRAM compute
element.</p>

<p>
GridFTP is also used to monitor the contents of files written by the
job during job execution. The standard GridFTP protocol is used by a
slightly unusual client to efficiently and reliably check the status
of files and incrementally fetch new content that is written to their
ends. This method supports "streaming" of file content from any file
accessible by GridFTP, rather than only the standard output and error
files named in the job request--the limitation of previous GRAM
solutions.
</p>

<p>The integration with GridFTP replaces the legacy GASS (Globus
Access to Secondary Storage) data transfer protocol. This changeover
is advantageous both for greater performance and reliability of data
staging as well as to remove redundant software from the GRAM
codebase.
</p>
</dd>

</dl>

<dl>

</p>

<p>
<dt>Delegation</dt>

<dd><p>The Delegation service of GT4 is used by clients to delegate
credentials into the correct hosting environment for use by WS_GRAM
or RFT services.</p>

<p>
The integration of the Delegation service replaces the implicit,
binding-level delegation of previous GRAM solutions with explicit
service operations. This change not only reduces the requirements on
client tooling for interoperability purposes, but also allows a new
separation of the lifecycle of jobs and delegated credentials.
Credentials can now be shared between multiple short-lived jobs or
eliminated entirely on an application-by-application basis to make
desired performance and security tradeoffs. Meanwhile, for unique
situations WS_GRAM retains the ability to refresh credentials for
long-lived jobs and gains an ability to designate different delegated
credentials for different parts of the job's lifecycle.
</p>
</dd>

</dl>

<h3>External Components used by WS_GRAM</h3>
<p>

<dl>

<dt>Local job scheduler</dt>

<dd><p>An optional local job scheduler is required in order to manage the
resources of the compute element. WS_GRAM has the ability to spawn
simple time-sharing jobs using standard Unix <tt>fork()</tt> methods,
but most large-scale compute elements will be under the control of a
schedule such as PBS, LSF, Loadleveler, etc.</p></dd>

<dt>sudo</dt>

<dd><p>The defacto standard Unix <tt>sudo</tt> utility is used by
WS_GRAM to gain access to target user accounts without requiring
WS_GRAM to have general super-user privelege on the system. The sudo
command is used to execute WS_GRAM adapter tools in the user account
context; these adapters perform the local system-specific operations
needed to initialize and run user jobs.</p>

<p>The <tt>sudo</tt> utility not only provides a simple way for
WS_GRAM to run programs as other users without "root" privelege, but
it provides fine-grained controls for the system administrator to
restrict which user accounts are valid WS_GRAM targets as well as
secure auditing of all operations requested by WS_GRAM.

This mechanism replaces the root-priveleged Gatekeeper component of
the Pre_WS_GRAM service in order to avoid running an entire WSRF
hosting environment as root.  This change provides enhanced security,
at the expense of slightly more complicated deployment effort, and is
motivated by the relative increase in the size of the WS_GRAM and WSRF
codebase as compared to the traditional Gatekeeper.</p>
</dd>
</dl>

</p>


<h2>Security Model</h2>

<h3>Secure operations</h3>

<p>WS_GRAM utilizes secure Web service invocation, as provided by the
WSRF core of the Globus Toolkit, for all job-management and
file-management messages. This security provides for authentication of
clients, tamper-resistant messaging, and optional privacy of message
content.</p>

<h3>Local system protection domains</h3>

<p>User jobs are executed within separate Unix user accounts. WS_GRAM
authentication mechanisms allow the administrator to control to which
local accounts a Grid-based client may submit jobs.  WS_GRAM uses the
Unix <tt>sudo</tt> command to access user accounts after determining
that the client has the right to access the account. Additionally, the
administrator may use access and allocation policy controls in the
local scheduler to further restrict the access of specific clients and
Unix users to local computing resources.</p>

<h3>Credential delegation and management</h3>

<p>A client may optionally delegate some of its rights to WS_GRAM and
related services in order to facilitate file staging. Additionally,
the client may delegate rights for use by the job process itself. If
no delegation is performed, staging is disallowed and the job will
have no ability to request priveleged Grid operations.</p>

<h3>Audit</h3>

<p>WS_GRAM provides three types of logging or auditing support:
<dl>

<dt>WSRF core message logging</dt>

<dd><p>Detailed logging of the underlying client messages may be logged
if such logging is enabled in the container configuration. (IS THIS
CORRECT? REF?)</p></dd>

<dt>WS_GRAM custom logging</dt>

<dd><p>WS_GRAM generates domain-specific logging information about job
requests and exceptional conditions</p></dd>

<dt>Local scheduler logging</dt>

<dd><p>For systems using a local batch scheduler, all of the
accounting and logging facilities of that scheduler remain available
for the administrator to track jobs whether submitted through WS_GRAM
or directly to the scheduler by local users.</p></dd>

<dt>Local system logging</dt>

<dd><p>The use of <tt>sudo</tt> for all operations against target user
accounts allows the administrator to log the low-level system
operations requested by WS_GRAM using <tt>sudo</tt>'s native auditing
support.</p></dd>

</dl>
</p>

</dl>

<h2>WS_GRAM Software Architecture</h2>

<h3>Overview</h3>

<img src="WS_GRAM_software.png">




<h3>Web services for Job Management</h3>

<p>The heart of the WS_GRAM service architecture is a set of Web
services designed to be hosted in the Globus Toolkit's WSRF core
hosting environment. Note, these services, described below, make use
of platform-specific callouts to other software components described
in the next section.</p>

<dl>

<dt>ManagedJob</dt>

<dd><p> Each submitted job is exposed as a distinct resource
qualifying the generic ManagedJob service. The service provides an
interface to monitor the status of the job or to terminate the job (by
terminating the ManagedJob resource).  The behavior of the service,
i.e. the local scheduler adapter implementation, is selected by the
specialized <em>type</em> of the resource.</p></dd>

<dt>ManagedJobFactory</dt>

<dd><p> Each compute element, as accessed through a local scheduler,
is exposed as a distinct resource qualifying the generic
ManagedJobFactory service. The service provides an interface to create
ManagedJob resources of the appropriate type in order to perform a job
in that local scheduler.</p></dd>

</dl>

<h3>Software for local system interaction</h3>

<dl>

<dt>Scheduler adapters</dt>

<dd><p>Support to control each local scheduler is provided in the form of
adapter scripts in the Perl programming language, following the
proprietary GRAM adapter plugin API.</p></dd>

<dt>gridmap_authorize_and_exec</dt>

<dd><p>The <tt>gridmap_authorize_and_exec</tt> tool is a default, but
optional, program that is invoked in the target user account as a
wrapper around WS_GRAM operations in order to make a final safety
check for whether WS_GRAM should be allowed to operate in that
account. This tool provides reasonable privelege limits to guard
against service compromise without requiring additional system
administrator efforts to manage user policies.</p></dd>

</dl>

<h2>Performance and Scalability</h2>

<h3>Protocol Variations</h3>

<p>
There are several optional parts to the WS_GRAM job workflow and
protocol. To understand the performance and scalability
characteristics of WS_GRAM, we must consider variations where
different parts of the protocol are used or omitted.
</p>

<p>
From a protocol perspective, the longest latency WS_GRAM submission
scenario involves a client delegating credentials for a job run that
will involve staging before and after the job and explicitly
terminating the job resource following job completion. (Note, the
credential refresh feature of WS_GRAM can be repeated any number of
times, so by longest sequence we mean the longest fixed sequence with
at most one credential delegation.)
</p>

<p>
To understand this figure depicting the protocol sequence, the arrows
show communication, signalling, or causal links between tiers in the
architecture and the vertical span indicates elapsed time (with the
start time at the top of the diagram). Also, we assume a user-supplied
job that can independently measure its start and end times as well as
checking in with the client so that the client may measure these times
(modulo messaging delays). Due to unspecified implementation delays,
client and job-observed times are not necessarily ordered with respect
to the WS_GRAM observed times and the WS_GRAM generated state
notification messages. The diagrams show one possible ordering but our
measurement methods must tolerate other orderings as well.
</p>

<img src="WS_GRAM_sequence_full.png">

<p>
All other performance scenarios discussed herein are subsets of this
sequence, i.e. where optional WS_GRAM features such as staging or
delegation are omitted.
</p>

<h4>Minimal Protocol Sequence</h4>

<p>
The simplest WS_GRAM scenario involves a job that requires neither
delegated credentials nor staging and that makes use of the automatic
termination of resources to avoid an explicit termination request.  In
this case, we can measure the latency and throughput for job
submission and notification alone.
</p>

<img src="WS_GRAM_sequence_minimal.png">

<p>
Note: Any difference between this case and the same measurement points in
the full scenario must be due to the additional overhead of the
delegation and staging services on the front-end node?
</p>

<h4>Non-staging Delegation Sequence</h4>

<p>
A slightly longer form of job than the minimal sequence is to include
credential delegation for use by the job itself, without any staging
directives. This sequence is comparable in functionality to previous
GRAM releases where delegation was mandatory but staging could be
omitted as per the client's request.
</p>

<img src="WS_GRAM_sequence_delegonly.png">

<h4>Staging Sequence</h4>

<p>
This full staging sequence uses almost all of the protocol elements,
omitting only the explicit termination step in favor of automatic
termination.
</p>

<img src="WS_GRAM_sequence_staging.png">

<h3>Pipelined Measurements</h3>

<p>
We report average performance for #TRIALS submissions of each of the
three variant sequences described above.  For these measurements, we
repeat the trials with differing numbers of concurrent (pipelined)
submissions from the same client. This leads to a relatively
steady-state measurement condition with a parameterized amount of
concurrent load.
</p>

<h4>Latency</h4>

<p>
The average latency of each sequence interval is reported for varying
levels of concurrency.
</p>

<h4>Throughput</h4>

<p>
The submission rate is reported for varying levels of concurrency.
</p>

<h3>Concurrency Limits</h3>

</body>
</html>
