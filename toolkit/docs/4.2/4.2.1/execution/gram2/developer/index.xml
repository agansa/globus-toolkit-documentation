<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd"
[

<!ENTITY % xinclude SYSTEM "../../../xinclude.mod" >
%xinclude;

]>

<book id="gram2Developer"><title>GT <replaceable role="entity">version</replaceable> GRAM2: Developer Guide</title>
<titleabbrev>Developer Guide</titleabbrev>
 
 
 <chapter id="gram2-developer-introduction"><title>Introduction</title>
   <para>
     There is no content available at this time.
   </para>
 </chapter>
 
 <chapter id="gram2-developer-public-interfaces"><title>Public interface</title>
   
   <section id="gram2-developer-seg"><title>Scheduler Event Generator</title>
   
     <para>
       <itemizedlist>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_scheduler_event_generator/html/main.html">
             General information</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_scheduler_event_generator/html/seg_protocol.html">
             SEG protocol</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_scheduler_event_generator/html/group__seg__api.html">
             SEG API</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_scheduler_event_generator_test/html/main.html">
             SEG Tests</ulink>
           </para>
         </listitem>
       </itemizedlist>
     </para>

   </section>
   
   <section id="gram2-developer-client"><title>Client</title>
 
     <para>
       <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_client/html/main.html">
             Client API</ulink>
     </para>
     
   </section>
   
   <section id="gram2-developer-protocol"><title>GRAM Proctocol</title>

     <para>
       <itemizedlist>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/main.html">
             General information</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/group__globus__gram__protocol__functions.html">
             GRAM Protocol Functions</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/group__globus__gram__protocol__job__state.html">
             Job States</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/group__globus__gram__protocol__job__signal.html">
             Signals</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/group__globus__gram__protocol__error.html">
             GRAM Errors</ulink>
           </para>
         </listitem>
         <listitem>
           <para>
             <ulink url="http://www.globus.org/api/c-globus-4.2.1/globus_gram_protocol/html/globus_gram_protocol.html">
             GRAM Protocol Message Format</ulink>
           </para>
         </listitem>
       </itemizedlist>
     </para>

   </section>   
   
 </chapter>  

 <chapter id="gram2-developer-tutorials"><title>Tutorials</title>
 
 <section id="globus_gram_job_manager_interface_tutorial"><title>GRAM Job Manager Scheduler Tutorial</title>
  
   <para>
     This tutorial describes the steps needed to build a GRAM Job Manager
     Scheduler interface package. The audience for this tutorial is a person
     interested in adding support for a new scheduler interface to GRAM. This
     tutorial will assume some familiarty with GTP, autoconf, automake, and
     Perl. As a reference point, this tutorial will refer to the code in the
     LSF Job Manager package.
   </para>

   <section id="job_manager_tutorial_service_writing"><title>Writing a Scheduler Interface</title>
     <para>
       This section deals with writing the perl module which implements the
       interface between the GRAM job manager and the local scheduler.
       Consult the <ulink url="#">Job Manager Scheduler Interface section</ulink>
       of this manual for a more detailed reference on the Perl modules
       which are used here.
     </para>
     <para>  
       The scheduler interface is implemented as a Perl module which is a
       subclass of the Globus::GRAM::JobManager module. Its name must match
       the scheduler type string used when the service is installed. For the
       LSF scheduler, the name is <emphasis>lsf</emphasis>, so the module name is
       <emphasis>Globus::GRAM::JobManager::lsf</emphasis> and it is stored in the
       file <computeroutput>lsf.pm</computeroutput>.
       Though there are several methods in the JobManager interface, they only
       ones which absolutely need to be implemented in a scheduler module are
       submit, poll, cancel.
     </para>
     <para>
       We'll begin by looking at the start of the lsf source module,
       lsf.in (the transformation to lsf.pm happens when the setup script is
       run. To begin the script, we import the GRAM support modules into the
       scheduler module's namespace, declare the module's namespace, and
       declare this module as a subclass of the Globus::GRAM::JobManager
       module. All scheduler packages will need to do this, substituting the
       name of the scheduler type being implemented where we see
       <emphasis>lsf</emphasis> below.
     </para>
     <screen>
use Globus::GRAM::Error;
use Globus::GRAM::JobState;
use Globus::GRAM::JobManager;
use Globus::Core::Paths;

...

package Globus::GRAM::JobManager::lsf;

@ISA = qw(Globus::GRAM::JobManager);
     </screen>
     <para>
       Next, we declare any system-specifc values which will be substituted
       when the setup package scripts are run. In the LSF case, we need the
       know the paths to a few programs which interact with the scheduler:
     </para>
     <screen>
BEGIN
{
    $mpirun = '@MPIRUN@';
    $bsub   = '@BSUB@';
    $bjobs  = '@BJOBS@';
    $bkill  = '@BKILL@';
}
     </screen>
     <para>
       The values surrounded by the at-sign (such as
       <computeroutput>@MPIRUN</computeroutput>) will be replaced by with
       the path to the named programs by the find-lsf-tools script described below.
     </para>
    

     <section id="gram_tut_constructor"><title>Writing a constructor</title>
       <para>
         For scheduler interfaces which need to setup some data before calling
         their other methods, they can overload the <computeroutput>new</computeroutput>
         method which acts as a constructor. Scheduler scripts which don't
         need any per-instance initialization will not need to provide a
         constructor, the Globus::GRAM::JobManager constructor will do the job.
       </para>
       <para>
         If you do need to overloaded this method, be sure to call the
         JobManager module's constructor to allow it to do its initialization,
         as in this example:
       </para>
       <screen>
sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class-&gt;SUPER::new(@_);

## Insert scheduler-specific startup code here
return $self;
}
       </screen>
       <para>
         The job interface methods are called with only one argument, the
         scheduler object itself. That object contains the a
         Globus::GRAM::JobDescription object (<computeroutput>$self-&gt;{JobDescription}
         </computeroutput>) which includes the values from the RSL string
         associated with the request, as well as a few extra values: 
       </para>
       <para><emphasis role="strong">job_id</emphasis></para>
       <para>
         The string returned as the value of JOB_ID in the return hash
         from submit. This won't be present for methods called before the job
         is submitted.
       </para>
       <para><emphasis role="strong">uniq_id</emphasis></para>
       <para>
         A string associated with this job request by the job manager program.
         It will be unique for all jobs on a host for all time.
       </para>
       <para><emphasis role="strong">cache_tag</emphasis></para>
       <para>
         The GASS cache tag related to this job submission. Files in the cache
         with this tag will be cleaned by the cleanup_cache() method.
       </para>  
       <para>
         Now, let's look at the methods which will interface to the scheduler.
       </para>
     
     </section>


     <section id="gram_tut_submitting_jobs"><title>Submitting Jobs</title>
     
       <para>
         All scheduler modules must implement the submit method. This method is
         called when the job manager wishes to submit the job to the scheduler.
         The information in the original job request RSL string is available to
         the scheduler interface through the
         <computeroutput>JobDescription</computeroutput> data member of it's hash.
       </para>
       <para>
         For most schedulers, this is the longest method to be implemented, as
         it must decide what to do with the job description, and convert them to
         something which the scheduler can understand.
       </para>
       <para>
         We'll look at some of the steps in the LSF manager code to see how the
         scheduler interface is implemented.
       </para>
       <para>
         In the beginning of the submit method, we'll get our parameters and
         look up the job description in the manager-specific object:
       </para>
       <screen>
sub submit
{
    my $self = shift;
    my $description = $self-&gt;{JobDescription};
       </screen>
       <para>
         Then we will check for values of the job parameters that we will be
         handling. For example, this is how we check for a valid job type in the
         LSF scheduler interface:
       </para>
       <screen>
if(defined($description-&gt;jobtype())
{
    if($description-&gt;jobtype !~ /^(mpi|single|multiple)$/)
    {
        return Globus::GRAM::Error::JOBTYPE_NOT_SUPPORTED;
    }
    elsif($description-&gt;jobtype() eq 'mpi' &amp;&amp; $mpirun eq "no")
    {
        return Globus::GRAM::Error::JOBTYPE_NOT_SUPPORTED;
    }
}
       </screen>
       <para>
         The lsf module supports most of the core RSL attributes, so it does
         more processing to determine what to do with the values in the job
         description.
       </para>
       <para>
         Once we've inspected the JobDescription we'll know what we need
         to tell the scheduler about so that it'll start the job properly. For
         LSF, we will construct a job description script and pass that to the
         <computeroutput>bsub</computeroutput> command. This script is a bourne
         shell script with some special comments which LSF uses to decide what
         constraints to use when scheduling the job.
       </para>
       <para>
         First, we'll open the new file, and write the file header:
       </para>
       <screen>
$lsf_job_script = new IO::File($lsf_job_script_name, '&gt;');

$lsf_job_script-&gt;print&lt;&lt;EOF;
#! /bin/sh
#
# LSF batch job script built by Globus Job Manager
#
EOF
       </screen>
       <para>
         Then, we'll add some special comments to pass job constraints to LSF:
       </para>
       <screen>
if(defined($queue))
{
    $lsf_job_script-&gt;print("#BSUB -q $queue\n");
}
if(defined($description-&gt;project()))
{
    $lsf_job_script-&gt;print("#BSUB -P " . $description-&gt;project() . "\n");
}
       </screen>
       <para>
         Before we start the executable in the LSF job description script, we
         will quote and escape the job's arguments so that they will be passed
         to the application as they were in the job submission RSL string:
       </para>
       <para>
         At the end of the job description script, we actually run the
         executable named in the JobDescription. For LSF, we support a few
         different job types which require different startup commands. Here, we
         will quote and escape the strings in the argument list so that the
         values of the arguments will be identical to those in the initial job
         request string. For this Bourne-shell syntax script, we will
         double-quote each argument, and escaping the backslash (\), dollar-sign
         ($), double-quote ("), and single-quote (') characters. We will use
         this new string later in the script.
       </para>
       <screen>
@arguments = $description-&gt;arguments();

foreach(@arguments)
{
    if(ref($_))
    {
         return Globus::GRAM::Error::RSL_ARGUMENTS;
    }
}
if($arguments[0])
{
    foreach(@arguments)
    {
         $_ =~ s/\\/\\\\/g;
         $_ =~ s/\$/\\\$/g;
         $_ =~ s"/\\\"/g;
         $_ =~ s/`/\\\`/g;

         $args .= '"' . $_ . '" ';
    }
}
else
{
    $args = "";
}
       </screen>
       <para>
         To end the LSF job description script, we will write the command
         line of the executable to the script. Depending on the job type of this
         submission, we will need to start either one or more instances of the
         executable, or the mpirun program which will start the job with the
         executable count in the <computeroutput>JobDescription</computeroutput>:
       </para>
       <screen>
if($description-&gt;jobtype() eq "mpi")
{
    $lsf_job_script-&gt;print("$mpirun -np " . $description-&gt;count() . " ");

    $lsf_job_script-&gt;print($description-&gt;executable()
                           . " $args \n");
}
elsif($description-&gt;jobtype() eq 'multiple')
{
    for(my $i = 0; $i &lt; $description-&gt;count(); $i++)
    {
        $lsf_job_script-&gt;print($description-&gt;executable() . " $args &amp;\n");
    }
    $lsf_job_script-&gt;print("wait\n");
}
else
{
    $lsf_job_script-&gt;print($description-&gt;executable() . " $args\n");
}
       </screen>
       <para>
         Next, we submit the job to the scheduler. Be sure to close the script
         file before trying to redirect it into the submit command, or some of
         the script file may be buffered and things will fail in strange ways!
       </para>
       <para>
         When the submission command returns, we check its output for
         the scheduler-specific job identifier. We will use this value to be
         able to poll or cancel the job.
       </para>
       <para>
         The return value of the script should be either a GRAM error
         object or a reference to a hash of values. The Globus::GRAM::JobManager
         documentation lists the valid keys to that hash. For the submit method,
         we'll return the job identifier as the value of JOB_ID in the hash. If
         the scheduler returned a job status result, we could return that as
         well. LSF does not, so we'll just check for the job ID and return it,
         or if the job fails, we'll return an error object:
       </para>
      <screen>
    $lsf_job_script-&gt;close();

    $job_id = (grep(/is submitted/,
                  split(/\n/, `$bsub &lt; $lsf_job_script_name`)))[0];
    if($? == 0)
    {
        $job_id =~ m/&lt;([^&gt;]*)&gt;/;
        $job_id = $1;

        return { JOB_ID =&gt; $job_id };
    }

    return Globus::GRAM::Error::INVALID_SCRIPT_REPLY;
}
       </screen>
       <para>
         That finishes the submit method. Most of the functionality for the
         scheduler interface is now written. We just have a few more (much
         shorter) methods to implement.
       </para>

     </section>

     
     <section id="gram_tut_polling"><title>Polling Jobs</title>
       
       <para>
         All scheduler modules must also implement the poll method. The purpose
         of this method is to check for updates of a job's status, for example,
         to see if a job has finished.
       </para>
       <para>
         When this method is called, we'll get the job ID (which we
         returned from the submit method above) as well as the original job
         request information in the object's JobDescription. In the LSF script,
         we'll pass the job ID to the <computeroutput>bjobs</computeroutput>
         program, and that will return the job's status information. We'll
         compare the status field from the <computeroutput>bjobs</computeroutput>
         output to see what job state we should return.
       </para>
       <para>
         If the job fails, and there is a way to determine that from the scheduler,
         then the script should return in its hash both
       </para>
       <screen>
JOB_STATE =&gt; Globus::GRAM::JobState::FAILED
       </screen>
       <para>
         and
       </para>
       <screen>
ERROR =&gt; Globus::GRAM::Error::&lt;ERROR_TYPE&gt;-&gt;value
       </screen>         
       <para>
         Here's an excerpt from the LSF scheduler module implementation:
       </para>
       <screen>
sub poll
{
    my $self = shift;
    my $description = $self-&gt;{JobDescription};
    my $job_id = $description-&gt;jobid();
    my $state;
    my $status_line;

    $self-&gt;log("polling job $job_id");

    # Get first line matching job id
    $_ = (grep(/$job_id/, `$bjobs $job_id 2&gt;/dev/null`))[0];

    # Get 3th field (status)
    $_ = (split(/\s+/))[2];

    if(/PEND/)
    {
        $state = Globus::GRAM::JobState::PENDING;
    }
    elsif(/USUSP|SSUSP|PSUSP/)
    {
        $state = Globus::GRAM::JobState::SUSPENDED
    }
    ...
    return {JOB_STATE =&gt; $state};
}
       </screen>

     </section>



     <section id="gram-tut-cancel"><title>Cancelling Jobs</title>

       <para>
         All scheduler modules must also implement the cancel method.
         The purpose of this method is to cancel a running job.
       </para>
       <para>
         As with the <computeroutput>poll</computeroutput> method described above, this
         method will be given the job ID as part of the JobDescription object
         held by the manager object. If the scheduler interface provides
         feedback that the job was cancelled successfully, then we can return a
         JOB_STATE change to the FAILED state. Otherwise we can return an empty
         hash reference, and let the poll method return the state change next
         time it is called.
       </para>
       <para>
         To process a cancel in the LSF case, we will run the bkill command with the job ID.
       </para>
       <screen>
sub cancel
{
    my $self = shift;
    my $description = $self-&gt;{JobDescription};
    my $job_id = $description-&gt;jobid();

    $self-&gt;log("cancel job $job_id");

    system("$bkill $job_id &gt;/dev/null 2&gt;/dev/null");

    if($? == 0)
    {
        return { JOB_STATE =&gt; Globus::GRAM::JobState::FAILED }
    }
    return Globus::GRAM::Error::JOB_CANCEL_FAILED;

}
       </screen>

     </section>
     
    
    
     <section id="gram_tut_exit_1"><title>End of the script</title>

       <para>
         It is required that all perl modules return a non-zero value
         when they are parsed. To do this, make sure the last line of your
         module consists of:
       </para>
       <screen>
1;
       </screen>
 
     </section>
   
   </section>
   
   
   
   
   <section id="job_manager_tutorial_service_setup"><title>Setting up a Scheduler</title>

     <para>
       Once we've written the job manager script, we need to get it
       installed so that the gatekeeper will be able to run our new service.
       We do this by writing a setup script. For LSF, we will write the script
       <computeroutput>setup-globus-job-manager-lsf.pl</computeroutput>, which
       we will list in the LSF package as the
       <emphasis role="strong">Post_Install_Program</emphasis>.
     </para>
     <para>
       To set up the Gatekeeper service, our LSF setup script does the following:
       <orderedlist>
         <listitem>
           <simpara>
             Perform system-specific configuration.
           </simpara>
         </listitem>
         <listitem>
           <simpara>
             Install the GRAM scheduler Perl module and register as a gatekeeper
             service.
           </simpara>
         </listitem>
         <listitem>
           <simpara>
             <emphasis role="strong">(Optional)</emphasis> Install an RSL
             validation file defining extra scheduler-specific RSL attributes
             which the scheduler interface will support.
           </simpara>
         </listitem>
         <listitem>
           <simpara>
             Update the GPT metadata to indicate that the job manager service
             has been set up.
           </simpara>
         </listitem>         
       </orderedlist>
     </para>  

     <section id="system_specific_configuration"><title>System-Specific Configuration</title>
       <para>
         First, our scheduler setup script probes for any system-specific
         information needed to interface with the local scheduler. For example,
         the LSF scheduler uses the <computeroutput>mpirun</computeroutput>,
         <computeroutput>bsub</computeroutput>, <computeroutput>bqueues</computeroutput>,
         <computeroutput>bjobs</computeroutput>, and <computeroutput>bkill</computeroutput>
         commands to submit, poll, and cancel jobs. We'll assume that the
         administrator who is installing the package has these commands in their
         path. We'll use an autoconf script to locate the executable paths for
         these commands and substitute them into our scheduler Perl module. In
         the LSF package, we have the <computeroutput>find-lsf-tools</computeroutput>
         script, which is generated during bootstrap by autoconf from the
         <computeroutput>find-lsf-tools.in</computeroutput> file:
       </para>
       <screen>
## Required Prolog

AC_REVISION($Revision: 1.2 $)
AC_INIT(lsf.in)

# checking for the GLOBUS_LOCATION

if test "x$GLOBUS_LOCATION" = "x"; then
    echo "ERROR Please specify GLOBUS_LOCATION" &gt;&amp;2
    exit 1
fi

...

## Check for optional tools, warn if not found

AC_PATH_PROG(MPIRUN, mpirun, no)
if test "$MPIRUN" = "no" ; then
    AC_MSG_WARN([Cannot locate mpirun])
fi

...

## Check for required tools, error if not found

AC_PATH_PROG(BSUB, bsub, no)
if test "$BSUB" = "no" ; then
    AC_MSG_ERROR([Cannot locate bsub])
fi

...

## Required epilog - update scheduler specific module

prefix='$(GLOBUS_LOCATION)'
exec_prefix='$(GLOBUS_LOCATION)'
libexecdir=${prefix}/libexec

AC_OUTPUT(
    lsf.pm:lsf.in
)
       </screen>
       <para>
         If this script exits with a non-zero error code, then the setup script
         propagates the error to the caller and exits without installing the
         service.
       </para>
     
     </section>
     
     <section id="installing_service_entry"><title>Registering as a Gatekeeper Service</title>

       <para>
         Next, the setup script installs it's perl module into the perl
         library directory and registers an entry in the Globus Gatekeeper's
         service directory. The program
         <ulink url="http://www-unix.globus.org/api/c-globus-2.4/perl/globus-job-manager-service.html">
         globus-job-manager-service</ulink>
         (distributed in the job manager program setup package) performs both of
         these tasks. When run, it expects the scheduler perl module to be
         located in the <computeroutput>$GLOBUS_LOCATION/setup/globus</computeroutput> directory.
       </para>
       <screen>
$libexecdir/globus-job-manager-service -add -m lsf -s jobmanager-lsf;
       </screen>

     </section>
     
     <section id="installing_validation_file"><title>Installing an RSL Validation File</title>

       <para>
         If the scheduler script implements RSL attributes which are not part of
         the core set supported by the job manager, it must publish them in the
         job manager's data directory. If the scheduler script wants to set some
         default values of RSL attributes, it may also set those as the default
         values in the validation file.
       </para>
       <para>
         The format of the validation file is described in th
         <ulink url="http://www-unix.globus.org/api/c-globus-2.4/globus_gram_job_manager/html/globus_gram_job_manager_rsl_validation_file.html#globus_gram_job_manager_rsl_validation_file">
         RSL Validation File Format</ulink> section of the documentation. The
         validation file must be named <emphasis>scheduler-type</emphasis>.rvf and
         installed in the
         <computeroutput>$GLOBUS_LOCATION/share/globus_gram_job_manager</computeroutput>
         directory.
       </para>
       <para>
         In the LSF setup script, we check the list of queues supported
         by the local LSF installation, and add a section of acceptable values
         for the <emphasis>queue</emphasis> RSL attribute:
       </para>
       <screen>
open(VALIDATION_FILE,
     "&gt;$ENV{GLOBUS_LOCATION}/share/globus_gram_job_manager/lsf.rvf");

# Customize validation file with queue info
open(BQUEUES, "bqueues -w |");

# discard header
$_ = &lt;BQUEUES&gt;;
my @queues = ();

while(&lt;BQUEUES&gt;)
{
    chomp;

    $_ =~ m/^(\S+)/;

    push(@queues, $1);
}
close(BQUEUES);

if(@queues)
{
    print VALIDATION_FILE "Attribute: queue\n";
    print VALIDATION_FILE join(" ", "Values:", @queues);

}
close VALIDATION_FILE;
       </screen>
       
     </section>
     
     
     <section id="updating_gpt_metadata"><title>Updating GPT Metadata</title>
       <para>
         Finally, the setup package should create and finalize a
         <computeroutput>Grid::GPT::Setup</computeroutput>. The value of
         $package must be the same value as the gpt_package_metadata
         <emphasis>Name</emphasis> attribute in the package's metadata file.
         If either the <computeroutput>new()</computeroutput> or
         <computeroutput>finish()</computeroutput> methods fail, then it is
         considered good practice to clean up any files created by the setup
         script. From
         <computeroutput>setup-globus-job-manager-lsf.pl</computeroutput>:
       </para>
       <screen>
my $metadata =
new Grid::GPT::Setup(
        package_name =&gt; "globus_gram_job_manager_setup_lsf");

...

$metadata-&gt;finish();
       </screen>
       
     </section>
     
   </section>



   <section id="job_manager_tutorial_service_packaging"><title>Packaging</title>

     <para>
       Now that we've written a job manager scheduler interface, we'll package
       it using GPT to make it easy for our users to build and install. We'll
       start by gathering the different files we've written above into a
       single directory <computeroutput>lsf</computeroutput>.
       <itemizedlist>
         <listitem>
           <simpara>
             lsf.in
           </simpara>
         </listitem>
         <listitem>
           <simpara>
             find-lsf-tools.in
           </simpara>
         </listitem>
         <listitem>
           <simpara>
             setup-globus-job-manager.pl
           </simpara>
         </listitem>      
       </itemizedlist>
     </para>
    
   
     <section id="job_manager_tutorial_doc"><title>Package Documentation</title>
   
       <para>
         If there are any scheduler-specific options defined for this scheduler
         module, or if there any any optional setup items, then it is good to
         provide a documentation page which describes these. For LSF, we
         describe the changes since the last version of this package in the file
         <computeroutput>globus_gram_job_manager_lsf.dox</computeroutput>. This
         file consists of a doxygen mainpage. See www.doxygen.org for information
         on how to write documentation with that tool.
       </para>
       
     </section>

     <section id="job_manager_tutorial_configure"><title>configure.in</title>
       
       <para>
         Now, we'll write our configure.in script. This file is converted to the
         configure shell script by the bootstrap script below. Since we don't do
         any probes for compile-time tools or system characteristics, we just
         call the various initialization macros used by GPT, declare that we may
         provide doxygen documentation, and then output the files we need
         substitions done on.
       </para>
       <screen>
AC_REVISION($Revision: 1.2 $)
AC_INIT(Makefile.am)

GLOBUS_INIT
AM_PROG_LIBTOOL

dnl Initialize the automake rules the last argument
AM_INIT_AUTOMAKE($GPT_NAME, $GPT_VERSION)

LAC_DOXYGEN("../", "*.dox")

GLOBUS_FINALIZE

AC_OUTPUT(
        Makefile
        pkgdata/Makefile
        pkgdata/pkg_data_src.gpt
        doxygen/Doxyfile
        doxygen/Doxyfile-internal
        doxygen/Makefile
)
       </screen>
     
     </section>

     <section id="job_manager_tutorial_pkg_data"><title>Package Metadata</title>
       
       <para>
         Now we'll write our metadata file, and put it in the pkgdata
         subdirectory of our package. The important things to note in this file
         are the package name and version, the post_install_program, and the
         setup sections. These define how the package distribution will be
         named, what command will be run by <computeroutput>gpt-postinstall</computeroutput>
         when this package is installed, and what the setup dependencies will be
         written when the Grid::GPT::Setup object is
         <ulink url="http://www-unix.globus.org/api/c-globus-2.4/globus_gram_job_manager/html/globus_gram_job_manager_interface_tutorial.html#updating_gpt_metadata">
         finalized</ulink>.
       </para>
       <screen>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE gpt_package_metadata SYSTEM "package.dtd"&gt;

&lt;gpt_package_metadata Format_Version="0.02" Name="globus_gram_job_manager_setup_lsf" &gt;

  &lt;Aging_Version Age="0" Major="1" Minor="0" /&gt;
  &lt;Description &gt;LSF Job Manager Setup&lt;/Description&gt;
  &lt;Functional_Group &gt;ResourceManagement&lt;/Functional_Group&gt;
  &lt;Version_Stability Release="Beta" /&gt;
  &lt;src_pkg &gt;

    &lt;With_Flavors build="no" /&gt;
    &lt;Source_Setup_Dependency PkgType="pgm" &gt;
      &lt;Setup_Dependency Name="globus_gram_job_manager_setup" &gt;
        &lt;Version &gt;
          &lt;Simple_Version Major="3" /&gt;
        &lt;/Version&gt;
      &lt;/Setup_Dependency&gt;
      &lt;Setup_Dependency Name="globus_common_setup" &gt;
        &lt;Version &gt;
          &lt;Simple_Version Major="2" /&gt;
        &lt;/Version&gt;
      &lt;/Setup_Dependency&gt;
    &lt;/Source_Setup_Dependency&gt;

    &lt;Build_Environment &gt;
      &lt;cflags &gt;@GPT_CFLAGS@&lt;/cflags&gt;
      &lt;external_includes &gt;@GPT_EXTERNAL_INCLUDES@&lt;/external_includes&gt;
      &lt;pkg_libs &gt; &lt;/pkg_libs&gt;
      &lt;external_libs &gt;@GPT_EXTERNAL_LIBS@&lt;/external_libs&gt;
    &lt;/Build_Environment&gt;

    &lt;Post_Install_Message &gt;
      Run the setup-globus-job-manager-lsf setup script to configure an
      lsf job manager.
    &lt;/Post_Install_Message&gt;

    &lt;Post_Install_Program &gt;
      setup-globus-job-manager-lsf
    &lt;/Post_Install_Program&gt;

    &lt;Setup Name="globus_gram_job_manager_service_setup" &gt;
      &lt;Aging_Version Age="0" Major="1" Minor="0" /&gt;
    &lt;/Setup&gt;

  &lt;/src_pkg&gt;

&lt;/gpt_package_metadata&gt;
       </screen>
     
     </section>
     
     <section id="job_manager_tutorial_automake"><title>Automake Makefile.am</title>

       <para>
         The automake Makefile.am for this package is short because there isn't
         any compilation needed for this package. We just need to define what
         needs to be installed into which directory, and what source files need
         to be put inot our source distribution. For the LSF package, we need to
         list the <computeroutput>lsf.in</computeroutput>,
         <computeroutput>find-lsf-tools</computeroutput>,
         and code&gt;setup-globus-job-manager-lsf.pl scripts as files to be
         installed into the setup directory. We need to add those files plus our
         documentation source file to the EXTRA_LIST variable so that they will
         be included in source distributions. The rest of the lines in the file
         are needed for proper interaction with GPT.
       </para>
       <screen>
include $(top_srcdir)/globus_automake_pre
include $(top_srcdir)/globus_automake_pre_top

SUBDIRS = pkgdata doxygen

setup_SCRIPTS = \
    lsf.in \
    find-lsf-tools \
    setup-globus-job-manager-lsf.pl

EXTRA_DIST = $(setup_SCRIPTS) globus_gram_job_manager_lsf.dox

include $(top_srcdir)/globus_automake_post
include $(top_srcdir)/globus_automake_post_top
       </screen>

     </section>


     <section id="job_manager_tutorial_bootstrap"><title>Bootstrap</title>

       <para>
         The final piece we need to write for our package is the
         <computeroutput>bootstrap</computeroutput> script. This script is
         the standard bootstrap script for a globus package, with an extra
         line to generate the <computeroutput>find-lsf-tools</computeroutput>
         script using autoconf.
       </para>
       <screen>
#!/bin/sh

# checking for the GLOBUS_LOCATION

if test "x$GLOBUS_LOCATION" = "x"; then
    echo "ERROR Please specify GLOBUS_LOCATION" &gt;&amp;2
    exit 1
fi

if [ ! -f ${GLOBUS_LOCATION}/libexec/globus-bootstrap.sh ]; then
    echo "ERROR: Unable to locate \${GLOBUS_LOCATION}/libexec/globus-bootstrap.sh"
    echo "       Please ensure that you have installed the globus-core package and"
    echo "       that GLOBUS_LOCATION is set to the proper directory"
    exit
fi

. ${GLOBUS_LOCATION}/libexec/globus-bootstrap.sh

autoconf find-lsf-tools.in &gt; find-lsf-tools
chmod 755 find-lsf-tools

exit 0
       </screen>

     </section>
   
   </section>
   
   
   
   
   
   
   <section id="job_manager_tutorial_building"><title>Building, Testing, and Debugging</title>
     
     <para>
       With this all done, we can now try to build our now package.
       To do so, we'll need to run
     </para>
     <screen>
% ./bootstrap
% ./gpt-build
     </screen>
     <para>
       If all of the files are written correctly, this should result in our
       package being installed into <computeroutput>$GLOBUS_LOCATION</computeroutput>.
       Once that is done, we should be able to run gpt-postinstall to configure
       our new job manager.
     </para>
     <para>
       Now, we should be able to run the command
     </para>
     <screen>
% globus-personal-gatekeeper -start -jmtype lsf
     </screen>
     <para>
       to start a gatekeeper configured to run a job manager using our new
       scripts. Running this will output a contact string (referred to as
       &lt;contact-string&gt; below), which we can use to connect to this new
       service. To do so, we'll run globus-job-run to submit a test job:
     </para>
     <screen>
% globus-job-run &lt;contact-string&gt; /bin/echo Hello, LSF
Hello, LSF
     </screen>


     <section id="job_manager_tutorial_debugging"><title>When Things Go Wrong</title>

       <para>
         If the test above fails, or more complicated job failures are
         occurring, then you'lll have to debug your scheduler interface. Here
         are a few tips to help you out.
       </para>
       <para>
         Make sure that your script is valid Perl. If you run
       </para>
       <screen>
perl -I$GLOBUS_LOCATION/lib/perl \
    $GLOBUS_LOCATION/lib/perl/Globus/GRAM/JobManager/lsf.pm
       </screen>
       <para>
         You should get no output. If there are any diagnostics, correct them
         (in the lsf.in file), reinstall your package, and rerun the setup
         script.
       </para>
       <para>
         Look at the <ulink url="http://www.globus.org/toolkit/docs/2.4/faq_errors.html">
         Globus Toolkit Error FAQ</ulink> and see if the failure is perhaps not
         related to your scheduler script at all.
       </para>
       <para>
         Enable logging for the job manager. By default, the job manager
         is configured to log only when it notices a job failure. However, if
         your problem is that your script is not returning a failure code when
         you expect, you might want to enable logging always. To do this, modify
         the job manager configuration file to contain
         "-save-logfile&nbsp;always" in place of "-save-log&nbsp;on_error".
       </para>
       <para>
         Adding logging messages to your script: the JobManager object implements
         a <computeroutput>log</computeroutput> method, which allows you to write
         messages to the job manager log file. Do this as your methods are called
         to pinpoint where the error occurs.
       </para>
       <para>
         Save the job description file when your script is run. This will allow
         you to run the
         <computeroutput>globus-job-manager-script.pl</computeroutput>
         interactively (or in the Perl debugger). To save the job description file,
         you can do
       </para>
       <screen>
$self-&gt;{JobDescription}-&gt;save("/tmp/job_description.$$");
       </screen>
       <para>
         in any of the methods you've implemented.
       </para>

     </section>
     
   </section>   

 </section>
 
 </chapter>

 <chapter id="gram2-developer-usage"><title>Usage scenarios</title>
   <para>
     There is no content available at this time.
   </para>
 </chapter>

 <chapter id="gram2-developer-debugging"><title>Debugging</title>
   <para>
     There is no content available at this time.
   </para>
 </chapter>

 <chapter id="gram2-developer-troubleshooting"><title>Troubleshooting</title>
   <para>
     There is no content available at this time.
   </para>
 </chapter>

 <chapter id="gram2-developer-related-doc"><title>Related Documentation</title>
   <para>
     Information about other C-APIs of the GT can be found
     <ulink url="http://www.globus.org/api/c-globus-4.2.1/">here</ulink>
   </para>
 </chapter>
</book>