<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">

<section id="gram4-qp-performance">
  <title>Performance reports</title>
  <indexterm type="gram4"><primary>performance</primary><secondary>reports</secondary></indexterm>

  <para>
    Tests had been run in 2 different environments: In a fully controlled VM cluster
    (Nomer cloud) and on a University of Chicago cluster (uct3-edge). We ran tests with
    a Throughput-tester program and Condor-G. The Throughput-tester is a Java program
    which can simulate all kinds of submission and job scenarios.
  </para>
  
  <section><title>Tests on Nomer</title>
    <para>
      Nomer is a cloud of max 6 VM's, each with the following characteristics
      <informaltable><tgroup cols="2">
        <tbody>
          <row>
            <entry><para><emphasis role="strong">Processor</emphasis></para></entry>
            <entry><para>Intel(R) Xeon(R) CPU E5430 @ 2.66GHz</para></entry>
          </row>
          <row>
            <entry><para><emphasis role="strong">RAM</emphasis></para></entry>
            <entry><para>2GB</para></entry>
          </row>
          <row>
            <entry><para><emphasis role="strong">OS</emphasis></para></entry>
            <entry><para>Linux/gentoo</para></entry>
          </row>
        </tbody>
      </tgroup></informaltable>
    </para>
    
    <section><title>Throughput-Tester</title>
      <para>
        5 client machines submitted to 1 server, each with 50 submission threads, for 1h.
        After 1h elapsed all clients waited for all jobs their to finish and terminated then.
        A client stopped submitting when it was holding 2K jobs in the server, and did
        continue submitting another job only after one of his jobs finished.
        This ensured that at most 10K jobs had been active in the container at any time.
      </para>
      <para>
        If a scenario included staging the transferred file had the size of 1B and transfers
        happened between each client VM and the server VM. If a scenario included fileCleanUp
        two files of size 1B had been deleted.
      </para>
      <para>
        On the server-side the jobs were run by 2 different users.
      </para>
      <informaltable border="0" cellspacing="0" cellpadding="0"><tgroup cols="10">
        <tbody>
          <row>
            <entry><para><emphasis role="strong">Monitoring</emphasis></para></entry>
            <entry><para><emphasis role="strong">Job Delegation</emphasis></para></entry>
            <entry><para><emphasis role="strong">Staging Delegation</emphasis></para></entry>
            <entry><para><emphasis role="strong">FileStageIn</emphasis></para></entry>
            <entry><para><emphasis role="strong">FileStageOut</emphasis></para></entry>
            <entry><para><emphasis role="strong">FileCleanUp</emphasis></para></entry>
            <entry><para><emphasis role="strong">API</emphasis></para></entry>
            <entry><para><emphasis role="strong">#Jobs</emphasis></para></entry>
            <entry><para><emphasis role="strong">Duration/min</emphasis></para></entry>
            <entry><para><emphasis role="strong">Comment</emphasis></para></entry>
          </row>
          <row>
            <entry><para>Polling 1pM</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>Stubs w/reuse</para></entry>
            <entry><para>14235</para></entry>
            <entry><para>184</para></entry>
            <entry><para>No heap settings</para></entry>
          </row>
          <row>
            <entry><para>Polling 1pM</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>--</para></entry>
            <entry><para>Stubs w/reuse</para></entry>
            <entry><para>14269</para></entry>
            <entry><para>181</para></entry>
            <entry><para>Heap settings: 256M-256M</para></entry>
          </row>
        </tbody>
      </tgroup></informaltable>
    </section>
  </section>
  
  <section><title>Tests on the uct3-edge cluster of the University of Chicago</title>  
    <para>
      The uct3-edge cluster, which was also used by other users at the time of the tests,
      has the following characteristics per machine:
      <informaltable><tgroup cols="2">
        <tbody>
          <row>
            <entry><para><emphasis role="strong">Processor</emphasis></para></entry>
            <entry><para>4 x Dual Core AMD Opteron(tm) Processor 285</para></entry>
          </row>
          <row>
            <entry><para><emphasis role="strong">RAM</emphasis></para></entry>
            <entry><para>8GB</para></entry>
          </row>
          <row>
            <entry><para><emphasis role="strong">OS</emphasis></para></entry>
            <entry><para>Linux/RHEL3</para></entry>
          </row>
        </tbody>
      </tgroup></informaltable>
    </para>
    <para>
      4 client machines (uct3-edge[2356]) submit to 1 server (uct3-edge7), each with 50 
      submission threads, for 1h. After 1h elapsed all clients await all jobs to finish												
      A client stops submitting when it is holding 2500 jobs in the server, and will continue
      submitting another job only after one of his jobs finished												
      This ensures that at most 10K jobs are active in the container at any time
      In case a scenario includes staging (stageIn == SI, stageOut == SO) the transferred
      file has the size of 1B
      In case a scenario includes fileCleanUp (FCU) two files of size 1B are deleted
      On the server-side the jobs are run by just one user, that is the same that runs the
      container. That means that no sudo callouts are done.        
    </para>
  </section>

</section>