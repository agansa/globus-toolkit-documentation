
<h1>GRAM configuration files</h1>

<ul>
  <li><a href="#configfilelocation">Locating configuration files</a></li>
  <li><a href="#serverconfig">Web service deployment configuration</a></li>
  <li><a href="#jndiconfig">JNDI application configuration</a></li>
  <li><a href="#securitydesc">Security descriptor</a> </li>
  <li><a href="#filesysmap">GRAM and GridFTP file system mapping</a></li>
</ul>


<h2><a name="configfilelocation"></a>Locating configuration files</h2>

All the GRAM configuration files are located in subdirectories of 
the <tt>$GLOBUS_LOCATION/etc</tt> directory. The names of the GRAM 
configuration directories all start with <tt>gram-service</tt>.
For instance, with a default GRAM installation, the command line:</p>
<pre>
% ls etc | grep gram-service
</pre>

gives the following output:

<pre>
gram-service
gram-service-Fork
gram-service-Multi
</pre>

<h2><a name="serverconfig"></a>Web service deployment configuration</h2>

The file <tt>$GLOBUS_LOCATION/etc/gram-service/server-config.wsdd</tt>
contains information necessary to deploy and instantiate the GRAM
services in the Globus container.
<p>
Three GRAM services are deployed:
<ul>
<li>ManagedExecutableJobService: service invoked when querying or
managing an <i>executable job</i> </li>

<li>ManagedMultiJobService: service invoked when querying or managing
a <i>multijob</i> </li>

<li>ManagedJobFactoryService: service invoked when submitting a job
</li>

</ul>

Each service deployment information contains the name of the Java
service implementation class, the path to the WSDL service file, the
name of the operation providers that the service reuses for its
implementation of WSDL-defined operations, etc. More information about
the service deployment configuration information can be found <a
href="../../common/javawscore/Java_WS_Core_Public_Interfaces.html#config">here</a>.

<h2><a name="jndiconfig"></a>JNDI application configuration</h2>

The configuration of WSRF resources and application-level service
configuration not related to service deployment is contained in <a
href="http://java.sun.com/products/jndi/">JNDI</a> files.  The
JNDI-based GRAM configuration is of two kinds:

<ul>
  <li><a href="#commonfactoryconfig">Common job factory configuration</a></li>
  <li><a href="#managerconfig">Local resource manager configuration</a></li>
</ul>

<h3><a name="commonfactoryconfig"></a>Common job factory configuration</h3>

The file <tt>$GLOBUS_LOCATION/etc/gram-service/jndi-config.xml</tt> contains 
configuration information that is common to every local resource manager.
<p>
More precisely, the configuration data it contains pertains to the implementation 
of the GRAM WSRF resources (factory resources and job resources), 
as well as initial values of WSRF resource properties that are always 
published by any Managed Job Factory WSRF resource. 
</p>
The data is categorized by service, because according to WSRF, 
in spite of the service/resource separation of concern, a given service 
will use only one XML Schema type of resource. In practice it is therefore clearer
to categorize the configuration resource implementation by service, 
even if theoretically speaking a given resource implementation could be 
used by several services. For more information, refer to the <a href="../../common/javawscore/index.html">Java
WS Core documentation</a>.
 <p>
Here is the decomposition, in JNDI objects, of the common configuration 
data, categorized by service. Each XYZHome object contains the same 
Globus Core-defined information for the implementation of the WSRF resource, 
such as the Java implementation class for the resource (<code>resourceClass</code> datum), 
the Java class for the resource key (<code>resourceKeyType</code> datum), etc. 

<ul>
<li>ManagedExecutableJobService
   <ul>
     <li>ManagedExecutableJobHome: configuration of the implementation of resources for the service.</li>
   </ul>
</li>

<li>ManagedMultiJobService
   <ul>
     <li>ManagedMultiJobHome: configuration of the implementation of resources for the service</li>
   </ul>
</li>

<li>ManagedJobFactoryService
   <ul>
   <li>FactoryServiceConfiguration: this encapsulates configuration information 
       used by the factory service. Currently this identifies the service to associate 
       to a newly created job resource in order to create an endpoint reference and 
       return it.
   </li>
   
   <li>ManagedJobFactoryHome: implementation of resources for the service
      resourceClass</li>
      
   <li>FactoryHomeConfiguration: this contains GRAM application-level configuration data 
       i.e. values for resource properties common to all factory resources. For instance, 
       the path to the Globus installation, host information such as CPU type, manufacturer, 
       operating system name and version, etc.</li>
   </ul>
</li>
 
</ul>     

<h3><a name="managerconfig"></a>Local resource manager configuration</h3>

When a SOAP call is made to a GRAM factory service in order to submit a job, 
the call is actually made to a GRAM service-resource pair, where the 
factory resource represents the local resource manager to be used to execute the job.
<p>
There is one directory <tt>gram-service-&lt;manager&gt;/</tt> 
for each local resource manager supported by the GRAM installation. 
</p>
<p>For instance, let's assume the command line:</p>
<pre>
% ls etc | grep gram-service-
</pre>

gives the following output:

<pre>
gram-service-Fork
gram-service-LSF
gram-service-Multi
</pre>
<p>In this example, the Multi, Fork and LSF job factory resources have been 
installed. <tt>Multi</tt> is a special kind of local resource manager 
which enables the GRAM services to support <a href="user/commandline_java_managed_globus_run.html#multijobs">multijobs</a>..
</p>


<p>The JNDI configuration file located under each manager directory 
contains configuration information for the GRAM support of the given 
local resource manager, such as the name that GRAM uses to designate 
the given resource manager. This is referred to as the <i>GRAM name</i> 
of the local resource manager.<p>
For instance, <tt>$GLOBUS_LOCATION/etc/gram-service-Fork/jndi-config.xml</tt> 
contains the following XML element structure:</p>
<pre>
    &lt;service name="ManagedJobFactoryService"&gt;
        &lt;!-- LRM configuration:  Fork --&gt;
        &lt;resource
            name="ForkResourceConfiguration"
            type="org.globus.exec.service.factory.FactoryResourceConfiguration"&gt;
            &lt;resourceParams&gt;
                [...]
                &lt;parameter&gt;
                    &lt;name&gt;
                        <b>localResourceManagerName</b>
                    &lt;/name&gt;
                    &lt;value&gt;
                        <b>Fork</b>
                    &lt;/value&gt;
                &lt;/parameter&gt;           
                &lt;parameter&gt;
                    &lt;name&gt;
                        <b>scratchDirectory</b>
                    &lt;/name&gt;
                    &lt;value&gt;
                        <b>${GLOBUS_USER_HOME}</b>
                    &lt;/value&gt;
                &lt;/parameter&gt;           
            &lt;/resourceParams&gt;
        &lt;/resource&gt;        
    &lt;/service&gt;
</pre>
<p>In the example above, the <i>GRAM name</i> of the local resource manager is <tt>Fork</tt>. 
This value can be used with the GRAM command line client in order to specify which factory resource to use when submitting a job.  Similarly, it is used to create contruct an endpoint reference to the chosen factory service-resource pair when using the GRAM client API. 
<br>
<p>In the example above, the <i>scratchDirectory</i> is set to <tt>${GLOBUS_USER_HOME}</tt>.  This is the default setting, it can be configured to point to an alternate netowrk file sustem path that is common to the compute cluster and is typically less reliable (auto purging), while offering a greater amount of disk space.  (e.g. /scratch)
</p>

<h2><a name="securitydesc"></a>Security descriptor</h2>

The file <tt>$GLOBUS_LOCATION/etc/gram-service/managed-job-factory-security-config.xml</tt> contains the Core security configuration for the GRAM <tt>ManagedJobFactory</tt> service:
<ul>
   <li>default security information for all remote invocations, such as:
       <ul>
	  <li>the authorization method, based on a Gridmap file (in order to resolve user credentials to local user names) </li>
	  <li>limited proxy credentials will be rejected</li>
       </ul>
   <li>security information for the <code>createManagedJob</code> operation</li>   
</ul>

The file <tt>$GLOBUS_LOCATION/etc/gram-service/managed-job-security-config.xml</tt> contains the Core security configuration for the GRAM job resources:
<ul>
    <li>The default is to only allow the identity that called the createManagedJob operation to access the resource.
    </li>
</ul>

Note: GRAM does not override the container security credentials defined in <tt>$GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml</tt>.  These are the credentials used to authenticate all service requests.


<h2><a name="filesysmap"></a>GRAM and GridFTP file system mapping</h2>
The file <tt>$GLOBUS_LOCATION/etc/gram-service/globus_gram_fs_map_config.xml</tt> 
contains information to associate local resource managers with GridFTP servers.  GRAM uses the GridFTP server (via RFT) to perform all file staging directives.
Since the GridFTP server and the Globus service container can be run on separate hosts, a mapping is needed between the common file system paths of these 2 hosts.
This enables the GRAM services to resolve file:/// staging directives to the local GridFTP URLs.
<p>

below is the default Fork entry.  mapping a jobPath of / to ftpPath of / will allow any file staging directive to be attempted.
<pre>
    &lt;map&gt;
        &lt;scheduler&gt;Fork&lt;/scheduler&gt;
        &lt;ftpServer&gt;
           &lt;protocol&gt;gsiftp&lt;/protocol&gt;
           &lt;host&gt;myhost.org&lt;/host&gt;
           &lt;port&gt;2811&lt;/port&gt;
        &lt;/ftpServer&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/&lt;/jobPath&gt;
           &lt;ftpPath&gt;/&lt;/ftpPath&gt;
        &lt;/mapping&gt;
    &lt;/map&gt;
</pre>

For a scheduler, where jobs will typically run on a compute node, a default entry is not provided.  This means staging directives will fail until a mapping is entered.
Here is an example of a compute cluster with PBS installed and has 2 common mount points between the front end host and the GridFTP server host.
<pre>
    &lt;map&gt;
        &lt;scheduler&gt;PBS&lt;/scheduler&gt;
        &lt;ftpServer&gt;
           &lt;protocol&gt;gsiftp&lt;/protocol&gt;
           &lt;host&gt;myhost.org&lt;/host&gt;
           &lt;port&gt;2811&lt;/port&gt;
        &lt;/ftpServer&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/pvfs/mount1/users&lt;/jobPath&gt;
           &lt;ftpPath&gt;/pvfs/mount2/users&lt;/ftpPath&gt;
        &lt;/mapping&gt;
        &lt;mapping&gt;
           &lt;jobPath&gt;/pvfs/jobhome&lt;/jobPath&gt;
           &lt;ftpPath&gt;/pvfs/ftphome&lt;/ftpPath&gt;
        &lt;/mapping&gt;
    &lt;/map&gt;
</pre>

The file system mapping schema doc is <a href="http://www-unix.globus.org/toolkit/docs/development/4.0-drafts/execution/wsgram/schemas/fs_mapping.html">here</a>
